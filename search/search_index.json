{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TechnoBureau: Unleashing the Power of Technology for a Connected Future","text":"<p>Welcome to TechnoBureau, the ultimate destination where technology enthusiasts, professionals, and the curious can delve into the ever-evolving world of innovation. As a premier online platform, TechnoBureau provides a wealth of information, news, and insights covering a broad spectrum of technology-related topics. Whether you're passionate about the latest trends, a seasoned tech professional, or simply eager to explore the realm of innovation, TechnoBureau is your one-stop hub to stay informed and satisfy your curiosity.</p> <p>Our commitment is to deliver comprehensive and engaging content that spans various areas of interest, including the latest devops tools, emerging technologies, software and apps, internet trends, cybersecurity, artificial intelligence, and much more. With a team of experienced writers and tech experts, we curate in-depth articles, insightful reviews, and thought-provoking analysis to ensure you have access to reliable and relevant information.</p> <p>TechnoBureau empowers you to stay ahead in the fields of Linux and DevOps, providing you with a trusted source for the latest trends, best practices, and advancements in these areas. Explore our extensive collection of articles, tutorials, and case studies that cover diverse topics such as Linux system administration, shell scripting, automation tools, cloud infrastructure, continuous integration and deployment, and beyond. Our expert writers and industry professionals offer deep insights, enabling you to make informed decisions and stay at the forefront of the rapidly evolving Linux and DevOps landscape.</p> <p>Unlock the full potential of container orchestration with our in-depth coverage of Kubernetes. Dive into the intricacies of Kubernetes architecture, deployment strategies, scalability, monitoring, and management techniques. TechnoBureau equips you with the knowledge and skills to confidently design, deploy, and manage scalable and resilient applications on Kubernetes, propelling your DevOps career to new heights.</p> <p>TechnoBureau is dedicated to supporting your career advancement in the Linux and DevOps domains. Our comprehensive guides, career-focused articles, and expert advice offer practical insights and strategies to help you achieve your professional goals. Whether you're aiming to land your dream job, enhance your technical skills, or transition into a DevOps role, TechnoBureau provides the guidance and resources you need to succeed.</p> <p>Engage, connect, and collaborate with a vibrant community of Linux and DevOps professionals. Our platform fosters networking, collaboration, and knowledge-sharing, providing a space for discussions, forums, and connections with like-minded individuals who share your passion for open-source technologies and DevOps practices. Exchange ideas, seek advice, and stay connected with the latest industry trends as part of our interactive community.</p> <p>Stay ahead of the curve and embark on an exciting technological journey with TechnoBureau. Join our ever-growing community and allow us to be your trusted guide in the fascinating world of technology. Explore, learn, and connect with the vibrant ecosystem of innovation as you navigate the vast possibilities offered by TechnoBureau.</p>"},{"location":"career/common/tell_me_about_yourself/","title":"Tell Me About Yourself","text":"","tags":["Career","Interview","Interview Preparation"]},{"location":"career/common/tell_me_about_yourself/#answer-1-product-manager-professional","title":"Answer 1 : Product Manager Professional","text":"<p>I would describe myself as highly curious and focused on learning in all parts of life, personal and professional.</p> <p>In my professional life, I look for the hardest problems to solve and where I can learn and develop the most. I\u2019ve taken on many different types of projects, including ads, virtual reality, commerce, and messaging. No matter what I\u2019m working on, I\u2019m very invested. I identify anyone I can learn from, as well as problems that I care about and try to optimize every step of the process.</p> <p>In my personal life, I spend a lot of time reading and usually have a focused area of interest for a longer period of time. For instance, last year, I read and researched public transportation systems and the future of transportation with emerging companies and autonomous vehicles. I found it fascinating, and it actually sparked a desire to change industries \u2014 which eventually led me to my last role and even helped prepare me for the switch.</p> <p>Outside of reading and researching, I also love to travel, cook with friends, and spend a lot of time running and being physically active outdoors.</p>","tags":["Career","Interview","Interview Preparation"]},{"location":"career/common/tell_me_about_yourself/#why-this-answer-worked-well","title":"Why this answer worked well:","text":"<ul> <li>Background and type of experience were clearly explained.</li> <li>Candidate showcased a self-starter mentality.</li> <li>Hobbies were framed to highlight benefits to professional life.</li> </ul>","tags":["Career","Interview","Interview Preparation"]},{"location":"career/common/tell_me_about_yourself/#answer-2-customer-management-professional","title":"Answer 2 : Customer Management Professional","text":"<p>From a very early age I've been a problem solver. I was that kid who would take apart anything so I could see how it worked\u2014and then try to put it back together.</p> <p>As you can imagine, it drove my parents nuts. But even though I tortured my family at times, the tinkering trait has served me well in my career.</p> <p>After graduating from Purdue, I was recruited into a field technician job and got paid to take apart broken packaging equipment. It was like living the dream.</p> <p>That job also made me realize I'm really good with difficult customers, and that's what helped me land my current account manager role.</p> <p>While I love my job and have been successful in it, it has moved me away from the manufacturing floor. Now, the reason I'm so interested in this position is that it seems to provide a really great blend of one-on-one work with clients and hands-on problem solving.</p>","tags":["Career","Interview","Interview Preparation"]},{"location":"career/common/tell_me_about_yourself/#why-this-answer-worked-well_1","title":"Why this answer worked well:","text":"<ul> <li>He/She gave a vivid image of his childhood home and told a memorable story about it.</li> <li>He/She picked two prominent required skills from the job description, problem solving and customer service, and built this interesting narrative around it.</li> <li>He/She showed how his career successfully evolved before he was even asked about that.</li> </ul>","tags":["Career","Interview","Interview Preparation"]},{"location":"devops/","title":"DevOps","text":"<p>we introduce you to the world of DevOps \u2013 a game-changing approach to software development and IT operations.</p> <p>DevOps, a fusion of \"Development\" and \"Operations,\" is a set of practices, tools, and cultural philosophies that promote collaboration and integration between development teams and operations teams. By breaking down traditional silos and fostering a shared responsibility for software delivery, DevOps enables organizations to deliver high-quality software faster, more reliably, and with greater efficiency.</p> <p>At its core, DevOps is built on several key principles. Collaboration and communication lie at the heart of DevOps, encouraging cross-functional teams to work closely together and share knowledge and insights. Automation plays a crucial role, enabling the streamlining of repetitive tasks, accelerating deployments, and ensuring consistency across environments. Continuous Integration (CI) and Continuous Deployment (CD) practices are employed, allowing for rapid and frequent software releases with minimal risk. Feedback loops are established to gather insights from users and stakeholders, driving continuous improvement and innovation. Lastly, DevOps emphasizes a culture of learning, experimentation, and resilience, enabling teams to adapt and respond to evolving business needs.</p> <p>Our organization is dedicated to helping businesses embrace the power of DevOps. We offer comprehensive services and solutions to guide organizations through their DevOps transformation journey. From strategy and planning to implementation and optimization, we empower companies to leverage DevOps principles to streamline their software development and operations, enhance collaboration, and deliver value to their customers more efficiently.</p> <p>By adopting DevOps, you can expect a range of benefits, including increased agility, reduced time-to-market, improved software quality, enhanced scalability, and better alignment between development and operations teams. These advantages lead to improved customer satisfaction, higher employee morale, and a competitive edge in today's fast-paced digital landscape.</p> <p>Join us as we embark on this transformative path of DevOps, where collaboration, automation, and continuous improvement converge to unlock the full potential of your software development and operations capabilities. Together, we'll revolutionize the way you deliver software and drive your organization towards success in the digital era.</p>","tags":["Learn","DevOps"]},{"location":"devops/introduction_about_terraform/","title":"Introduction About Terraform","text":"<p>Terraform is an open-source Infrastructure as Code(IaC) tool developed by HashiCorp. It is used to define and provision the complete infrastructure using an easy-to-learn declarative language.</p> <p>It is an infrastructure provisioning tool where you can store your cloud infrastructure setup as codes. It's very similar to tools such as CloudFormation, which you would use to automate your AWS infrastructure, but you can only use that on AWS. With Terraform, you can use it on other cloud platforms as well.</p> <p>Below are some of the benefits of using Terraform.</p> <ul> <li>Does orchestration, not just configuration management</li> <li>Supports multiple providers such as AWS, Azure, GCP, DigitalOcean     and many more</li> <li>Provide immutable infrastructure where configuration changes     smoothly</li> <li>Uses easy to understand language, HCL (HashiCorp configuration     language)</li> <li>Easily portable to any other provider</li> <li>Supports Client only architecture, so no need for additional     configuration management on a server</li> </ul> <p>Note</p> <p>Infrastructure as Code (IaC) is a widespread terminology among DevOps   professionals. It is the process of managing and provisioning the   complete IT infrastructure (comprises both physical and virtual   machines) using machine-readable definition files. It is a software   engineering approach toward operations. It helps in automating the   complete data center by using programming scripts.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#installation-of-terraform","title":"Installation of Terraform","text":"Ubuntu/DebianCentOS/RHEL Bash Session<pre><code>$ curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\n$ sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\n$ sudo apt-get update &amp;&amp; sudo apt-get install terraform\n</code></pre> Bash Session<pre><code>$ sudo yum install -y yum-utils\n$ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\n$ sudo yum -y install terraform\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#terraform-core-concepts","title":"Terraform Core concepts","text":"<p>Terraform core uses two input sources to do its job.</p> <p>The first input source is a Terraform configuration that you, as a user, configure. Here, you define what needs to be created or provisioned. And the second input source is a state where terraform keeps the up-to-date state of how the current set up of the infrastructure looks like.</p> <p>So, what terraform core does is it takes the input, and it figures out the plan of what needs to be done. It compares the state, what is the current state, and what is the configuration that you desire in the end result. It figures out what needs to be done to get to that desired state in the configuration file. It figures what needs to be created, what needs to be updated, what needs to be deleted to create and provision the infrastructure.</p> <p></p> <p>The second component of the architecture are providers for specific technologies. This could be cloud providers like AWS, Azure, GCP, or other infrastructure as a service platform. It is also a provider for more high-level components like Kubernetes or other platform-as-a-service tools, even some software as a self-service tool.</p> <p>It gives you the possibility to create infrastructure on different levels.</p> <p>For example - create an AWS infrastructure, then deploy Kubernetes on top of it and then create services/components inside that Kubernetes cluster.</p> <p>Terraform has over a hundred providers for different technologies, and each provider then gives terraform user access to its resources. So through AWS provider, for example, you have access to hundreds of AWS resources like EC2 instances, the AWS users, etc. With Kubernetes provider, you access to commodities, resources like services and deployments and namespaces, etc.</p> <p>Below are the core concepts/terminologies used in Terraform:</p> <ul> <li> <p>Provider: It is a plugin to interact with APIs of service and     access its related resources.</p> </li> <li> <p>Resources: It refers to a block of one or more infrastructure     objects (compute instances, virtual networks, etc.), which are used     in configuring and managing the infrastructure.</p> </li> <li> <p>count and for_each Meta Arguments: - It allow us to create     multiple instances of any resource.</p> </li> <li> <p>Data Source: It is implemented by providers to return     information on external objects to terraform.</p> </li> <li> <p>State: It consists of cached information about the     infrastructure managed by Terraform and the related configurations.</p> </li> <li> <p>Module: It is a folder with Terraform templates where all the     configurations are defined</p> </li> <li> <p>Input Variables: It is key-value pair used by Terraform modules     to allow customization.</p> </li> <li> <p>Local Variables: It work like standard variables,but their scope is     limited to the module where they're declared.</p> </li> <li> <p>Output Values: These are return values of a terraform module     that can be used by other configurations.</p> </li> </ul>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#provider","title":"Provider","text":"<p>A provider works pretty much as an operating system's device driver. It exposes a set of resource types using a common abstraction, thus masking the details of how to create, modify, and destroy a resource pretty much transparent to users.</p> <p>Terraform downloads providers automatically from its public registry as needed, based on the resources of a given project. It can also use custom plugins, which must be manually installed by the user. Finally, some built-in providers are part of the main binary and are always available.</p> <p>Although not strictly necessary, it's considered a good practice to explicitly declare which provider we'll use in our Terraform project and inform its version. For this purpose, we use the version attribute available to any provider declaration:</p> Terraform<pre><code>provider \"aws\" {\n   access_key = \"B5KG6Fe5GUKIATUF5UD\"\n   secret_key = \"R4gb65y56GBF6765ejYSJA4YtaZ+T6GY7H\"\n   region = \"us-east-2\"\n}\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#resources","title":"Resources","text":"<p>In Terraform, a resource is anything that can be a target for CRUD operations in the context of a given provider. Some examples are an EC2 instance, an Azure MariaDB, or a DNS entry.</p> <p>Let's look at a simple resource definition:</p> Terraform<pre><code>resource \"aws_instance\" \"web\" {\n   ami = \"some-ami-id\"\n   instance_type = \"t2.micro\"\n}\n</code></pre> <p>First, we always have the resource keyword that starts a definition. Next, we have the resource type, which usually follows the provider_type convention. In the above example, aws_instance is a resource type defined by the AWS provider, used to define an EC2 instance. After that, there's the user-defined resource name, which must be unique for this resource type in the same module -- more on modules later.</p> <p>Finally, we have a block containing a series of arguments used as a resource specification. A key point about resources is that once created, we can use expressions to query their attributes. Also, and equally important, we can use those attributes as arguments for other resources.</p> <p>To illustrate how this works, let's expand the previous example by creating our EC2 instance in a non-default VPC</p> Terraform<pre><code>resource \"aws_vpc\" \"apps\" {\n   cidr_block = \"10.0.0.0/16\"\n}\n\nresource \"aws_subnet\" \"frontend\" {\n   vpc_id = \"${aws_vpc.apps.id}\"\n   cidr_block = \"10.0.1.0/24\"\n}\nresource \"aws_instance\" \"web\" {\n   ami = \"some-ami-id\"\n   instance_type = \"t2.micro\"\n   subnet_id = \"${aws_subnet.frontend.id}\"\n}\n</code></pre> <p>Here, we use the id attribute from our VPC resource as the value for the frontend's vpc_id argument. Next, its id parameter becomes the argument to the EC2 instance. Please note that this particular syntax requires Terraform version 0.12 or later. Previous versions used a more cumbersome \"${expression}\" syntax, which is still available but considered legacy.</p> <p>This example also shows one of Terraform's strengths: regardless of the order in which we declare resources in our project, it will figure out the correct order in which it must create or update them based on a dependency graph it builds when parsing them.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#count-and-for_each-meta-arguments","title":"count and for_each Meta Arguments","text":"<p>It allow us to create multiple instances of any resource. The main difference between them is that count expects a non-negative number, whereas for_each accepts a list or map of values.</p> <p>For instance, let's use count to create some EC2 instances on AWS:</p> Terraform<pre><code>resource \"aws_instance\" \"server\" {\n   count = \"${var.server_count}\"\n   ami = \"ami-xxxxxxx\"\n   instance_type = \"t2.micro\"\n   tags = {\n      Name = \"WebServer - ${count.index}\"\n   }\n}\n</code></pre> <p>Within a resource that uses count, we can use the count object in expressions. This object has only one property: index, which holds the index (zero-based) of each instance.</p> <p>Likewise, we can use the for_each meta argument to create those instances based on a map:</p> Terraform<pre><code>variable \"instances\" {\n   type = map(string)\n}\nresource \"aws_instance\" \"server\" {\n   for_each = \"${var.instances}\"\n   ami = \"${each.value}\"\n   instance_type = \"t2.micro\"\n   tags = {\n      Name = \"${each.key}\"\n   }\n}\n</code></pre> <p>This time, we've used a map from labels to AMI (Amazon Machine Image) names to create our servers. Within our resource, we can use the each object, which gives us access to the current key and value for a particular instance.</p> <p>A key point about count and for_each is that, although we can assign expressions to them, Terraform must be able to resolve their values before performing any resource action. As a result, we cannot use an expression that depends on output attributes from other resources.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#data-sources","title":"Data Sources","text":"<p>Data sources work pretty much as \"read-only\" resources, in the sense that we can get information about existing ones but can't create or change them. They are usually used to fetch parameters needed to create other resources.</p> <p>A typical example is the aws_ami data source available in the AWS provider, which we use to recover attributes from an existing AMI:</p> Terraform<pre><code>data \"aws_ami\" \"ubuntu\" {\n   most_recent = true\n\n   filter {\n      name   = \"name\"\n      values = [\"ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*\"]\n   }\n\n   filter {\n      name   = \"virtualization-type\"\n      values = [\"hvm\"]\n   }\n   owners = [\"099720109477\"]\n}\n</code></pre> <p>This example defines a data source called \"ubuntu\" that queries the AMI registry and returns several attributes related to the located image. We can then use those attributes in other resource definitions, prepending the data prefix to the attribute name:</p> Terraform<pre><code>resource \"aws_instance\" \"web\" {\n   ami = \"${data.aws_ami.ubuntu.id}\"\n   instance_type = \"t2.micro\"\n}\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#state","title":"State","text":"<p>The state of a Terraform project is a file that stores all details about resources that were created in the context of a given project. For instance, if we declare an azure_resourcegroup resource in our project and run Terraform, the state file will store its identifier.</p> <p>The primary purpose of the state file is to provide information about already existing resources, so when we modify our resource definitions, Terraform can figure out what it needs to do.</p> <p>An important point about state files is that they may contain sensitive information. Examples include initial passwords used to create a database, private keys, and so on.</p> <p>Terraform uses the concept of a backend to store and retrieve state files. The default backend is the local backend, which uses a file in the project's root folder as its storage location. We can also configure an alternative remote backend by declaring it in a terraform block in one of the project's .tf files:</p> Terraform<pre><code>terraform {\nbackend \"s3\" {\n   bucket = \"some-bucket\"\n   key = \"some-storage-key\"\n   region = \"us-east-1\"\n}\n}\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#modules","title":"Modules","text":"<p>Terraform modules are the main feature that allows us to reuse resource definitions across multiple projects or simply have a better organization in a single project. This is much like what we do in standard programming: instead of a single file containing all code, we organize our code across multiple files and packages.</p> <p>A module is just a directory containing one or more resource definition files. In fact, even when we put all our code in a single file/directory, we're still using modules - in this case, just one. The important point is that sub-directories are not included as part of a module. Instead, the parent module must explicitly include them using the module declaration:</p> Terraform<pre><code>module \"networking\" {\n   source = \"./networking\"\n   create_public_ip = true\n}\n</code></pre> <p>Here we're referencing a module located at the \"networking\" sub-directory and passing a single parameter to it - a boolean value in this case.</p> <p>It's important to note that in its current version, Terraform does not allow the use of count and for_each to create multiple instances of a module.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#input-variables","title":"Input Variables","text":"<p>Any module, including the top, or main one, can define several input variables using variable block definitions:</p> Terraform<pre><code>variable \"myvar\" {\n   type = string\n   default = \"Some Value\"\n   description = \"MyVar description\"\n}\n</code></pre> <p>A variable has a type, which can be a string, map, or set, among others. It also may have a default value and description. For variables defined at the top-level module, Terraform will assign actual values to a variable using several sources:</p> <ul> <li>-var command-line option</li> <li>.tfvar files, using command-line options or scanning for well-known     files/locations</li> <li>Environment variables starting with TF_VAR_</li> <li>The variable's default value, if present</li> </ul> <p>As for variables defined in nested or external modules, any variable that has no default value must be supplied using arguments in a module reference. Terraform will generate an error if we try to use a module that requires a value for an input variable but we fail to supply one.</p> <p>Once defined, we can use variables in expressions using the var prefix:</p> Terraform<pre><code>resource \"xxx_type\" \"some_name\" {\n   arg = \"${var.myvar}\"\n}\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#output-values","title":"Output Values","text":"<p>By design, a module's consumer has no access to any resources created within the module. Sometimes, however, we need some of those attributes to use as input for another module or resource. To address those cases, a module can define output blocks that expose a subset of the created resources:</p> Terraform<pre><code>output \"web_addr\" {\n   value = \"${aws_instance.web.private_ip}\"\n   description = \"Web server's private IP address\"\n}\n</code></pre> <p>Here we're defining an output value named \"web_addr\" containing the IP address of an EC2 instance that our module created. Now any module that references our module can use this value in expressions as module.module_name.web_addr, where module_name is the name we've used in the corresponding module declaration.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#local-variables","title":"Local Variables","text":"<p>Local variables work like standard variables, but their scope is limited to the module where they're declared. The use of local variables tends to reduce code repetition, especially when dealing with output values from modules:</p> Terraform<pre><code>locals {\n   vpc_id = \"${module.network.vpc_id}\"\n}\nmodule \"network\" {\n   source = \"./network\"\n}\nmodule \"service1\" {\n   source = \"./service1\"\n   vpc_id = \"${local.vpc_id}\"\n}\nmodule \"service2\" {\n   source = \"./service2\"\n   vpc_id = \"${local.vpc_id}\"\n}\n</code></pre> <p>Here, the local variable vpc_id receives the value of an output variable from the network module. Later, we pass this value as an argument to both service1 and service2 modules.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#workspaces","title":"Workspaces","text":"<p>Terraform workspaces allow us to keep multiple state files for the same project. When we run Terraform for the first time in a project, the generated state file will go into the default workspace. Later, we can create a new workspace with the terraform workspace new command, optionally supplying an existing state file as a parameter.</p> <p>We can use workspaces pretty much as we'd use branches in a regular VCS. For instance, we can have one workspace for each target environment - DEV, QA, PROD - and, by switching workspaces, we can terraform apply changes as we add new resources.</p> <p>Given the way this works, workspaces are an excellent choice to manage multiple versions - or \"incarnations\" if you like - of the same set of configurations. This is great news for everyone who's had to deal with the infamous \"works in my environment\" problem, as it allows us to ensure that all environments look the same.</p> <p>In some scenarios, it may be convenient to disable the creation of some resources based on the particular workspace we're targeting. For those occasions, we can use the terraform.workspace predefined variable. This variable contains the name of the current workspace, and we can use it as any other in expressions.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#terraform-lifecycle","title":"Terraform Lifecycle","text":"<p>Terraform lifecycle consists of - init, plan, apply, and destroy.</p> <p></p> <ul> <li>terraform init initializes the working directory which consists     of all the configuration files</li> <li>terraform plan is used to create an execution plan to reach a     desired state of the infrastructure. Changes in the configuration     files are done in order to achieve the desired state.</li> <li>terraform apply then makes the changes in the infrastructure as     defined in the plan, and the infrastructure comes to the desired     state.</li> <li>terraform destroy is used to delete all the old infrastructure     resources, which are marked tainted after the apply phase.</li> </ul>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#terraform-init","title":"terraform init","text":"<p>Let us look into small terraform code to execute and understand the basics.</p> <p>Create main.tf and add the below contents into the main.tf file to understand about the terraform lifecycle.</p> Terraform<pre><code>resource \"local_file\" \"hello\" {\n   content              = \"Hello, Terraform\"\n   directory_permission = \"0777\"\n   file_permission      = \"0777\"\n   filename             = \"hello.txt\"\n}\n</code></pre> <p>Since this is the first time we're running this project, we need to initialize it with the init command:</p> Bash Session<pre><code>$ terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Checking for available provider plugins...\n- Downloading plugin for provider \"local\" (hashicorp/local) 2.2.3...\n\nTerraform has been successfully initialized!\n... more messages omitted\n</code></pre> <p>In this step, Terraform scans our project files and downloads any required provider -- the local provider, in our case.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#terraform-plan","title":"terraform plan","text":"<p>we use the plan command to verify what actions Terraform will perform to create our resources. This step works pretty much as the \"dry run\" feature available in other build systems, such as GNU's make tool:</p> Bash Session<pre><code>$ terraform plan\n... messages omitted\nTerraform will perform the following actions:\n\n# local_file.hello will be created\n+ resource \"local_file\" \"hello\" {\n      + content              = \"Hello, Terraform\"\n      + directory_permission = \"0777\"\n      + file_permission      = \"0777\"\n      + filename             = \"hello.txt\"\n      + id                   = (known after apply)\n   }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n... messages omitted\n</code></pre> <p>Here, Terraform is telling us that it needs to create a new resource, which is expected as it doesn't exist yet. We can also see the provided values we've set and a pair of permission attributes. As we haven't supplied those in our resource definition, the provider will assume default values.</p>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#terraform-apply","title":"terraform apply","text":"<p>We can now proceed to actual resource creation using the apply command:</p> Bash Session<pre><code>$ terraform apply\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n+ create\n\nTerraform will perform the following actions:\n\n# local_file.hello will be created\n+ resource \"local_file\" \"hello\" {\n      + content              = \"Hello, Terraform\"\n      + directory_permission = \"0777\"\n      + file_permission      = \"0777\"\n      + filename             = \"hello.txt\"\n      + id                   = (known after apply)\n   }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\nTerraform will perform the actions described above.\nOnly 'yes' will be accepted to approve.\n\nEnter a value: yes\n\nlocal_file.hello: Creating...\nlocal_file.hello: Creation complete after 0s [id=392b5481eae4ab2178340f62b752297f72695d57]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n</code></pre> <p>We can now verify that the file has been created with the specified content:</p> Bash Session<pre><code>$ cat hello.txt\nHello, Terraform\n</code></pre> <p>All good! Now, let's see what happens if we rerun the apply command, this time using the -auto-approve flag so Terraform goes right away without asking for any confirmation:</p> Bash Session<pre><code>$ terraform apply -auto-approve\nlocal_file.hello: Refreshing state... [id=392b5481eae4ab2178340f62b752297f72695d57]\n\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\n</code></pre> <p>This time, Terraform did nothing because the file already existed. That's not all, though. Sometimes a resource exists, but someone may have changed one of its attributes, a scenario that is usually referred to as \"configuration drift\". Let's see how Terraform behaves in this scenario:</p> Bash Session<pre><code>$ echo foo &gt; hello.txt\n$ terraform plan\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\nlocal_file.hello: Refreshing state... [id=392b5481eae4ab2178340f62b752297f72695d57]\n\n------------------------------------------------------------------------\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n+ create\n\nTerraform will perform the following actions:\n\n# local_file.hello will be created\n+ resource \"local_file\" \"hello\" {\n      + content              = \"Hello, Terraform\"\n      + directory_permission = \"0777\"\n      + file_permission      = \"0777\"\n      + filename             = \"hello.txt\"\n      + id                   = (known after apply)\n   }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n... more messages omitted\n</code></pre> <p>Terraform has detected the change in the hello.txt file's content and generated a plan to restore it. Since the local provider lacks supports for in-place modification, we see that the plan consists of a single step -- recreating the file.</p> <p>We now can run apply again and, as a result, it will restore the contents of the file to its intended content:</p> Bash Session<pre><code>$ terraform apply -auto-approve\n... messages omitted\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\n$ cat hello.txt\nHello, Terraform\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/introduction_about_terraform/#terraform-destroy","title":"terraform destroy","text":"<p>It would destroy the resources which we created from current directory and according to the state file.</p> Bash Session<pre><code>$ terraform destroy --auto-approve\n... more messages omitted\n\nTerraform will perform the following actions:\n\n# local_file.hello will be destroyed\n- resource \"local_file\" \"hello\" {\n      - content              = \"Hello, Terraform\" -&gt; null\n      - directory_permission = \"0777\" -&gt; null\n      - file_permission      = \"0777\" -&gt; null\n      - filename             = \"hello.txt\" -&gt; null\n      - id                   = \"392b5481eae4ab2178340f62b752297f72695d57\" -&gt; null\n   }\n\nPlan: 0 to add, 0 to change, 1 to destroy.\n... more messages omitted\n</code></pre>","tags":["Terraform","DevOps","Terraform Basics","Terraform Tutorial"]},{"location":"devops/step_by_step_guide_about_docker_image_build/","title":"Building Docker Images - A Step-by-Step Guide with Examples","text":"","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#what-is-docker","title":"What is Docker?","text":"<p>Docker is an open-source platform that enables the creation, deployment, and management of applications within lightweight, isolated containers. It provides a consistent and efficient way to package software and its dependencies, ensuring that applications run reliably across different computing environments.</p> <p>At its core, Docker utilizes containerization technology, which allows applications to be packaged into self-contained units called Docker containers. Each container contains everything needed to run an application, including the code, runtime, system tools, libraries, and system dependencies. Containers are isolated from one another and share the host system's operating system kernel, making them lightweight and efficient.</p> <p>Docker offers several key advantages:</p> <ol> <li> <p>Portability: Docker containers are portable, meaning they can run consistently across different environments, such as development, testing, and production. This eliminates the \"it works on my machine\" problem and ensures application consistency.</p> </li> <li> <p>Scalability: Docker enables easy scaling of applications by allowing multiple instances of containers to run concurrently. Containers can be quickly and dynamically deployed, making it straightforward to scale up or down based on demand.</p> </li> <li> <p>Resource Efficiency: Docker containers are lightweight and share the host system's resources, making efficient use of hardware resources. Multiple containers can run on the same physical or virtual machine without conflicts.</p> </li> <li> <p>Isolation: Docker containers provide isolation, ensuring that each application runs independently of others. This isolation enhances security and prevents conflicts between applications or their dependencies.</p> </li> <li> <p>Version Control: Docker images, which are the building blocks of containers, can be version-controlled. This allows for easy management of different versions and facilitates rollbacks if needed.</p> </li> <li> <p>Rapid Deployment: Docker simplifies the deployment process by providing a standardized environment for applications. With Docker, applications can be deployed quickly and consistently, reducing deployment time and effort.</p> </li> </ol> <p>Docker has gained widespread adoption and is widely used in various scenarios, including microservices architecture, continuous integration and deployment (CI/CD), and cloud-based environments. It has a vast ecosystem of tools and services that support container orchestration, networking, storage, and management, making it a popular choice for modern application development and deployment.</p>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#installation-configuration-of-docker","title":"Installation &amp; Configuration of Docker","text":"<p>To install Docker Engine on a new host machine, it is important to first set up the Docker repository. Once the repository is configured, you can proceed with installing and updating Docker using the repository.</p>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#set-up-the-repository","title":"Set up the repository","text":"<p>It provide instructions to set up the necessary repository and prerequisites for installing the Docker engine based on the operating system (OS) in use.</p> Ubuntu/DebianCentOS/RHEL Bash Session<pre><code>$ sudo apt-get update\n$ sudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n$ sudo mkdir -p /etc/apt/keyrings\n$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n$ echo \\\n    \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \\\n    $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> Bash Session<pre><code>$ sudo yum install -y yum-utils\n$ sudo yum-config-manager \\\n          --add-repo \\\n          https://download.docker.com/linux/centos/docker-ce.repo\n</code></pre>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#install-docker-engine","title":"Install Docker Engine","text":"<p>Install docker engine based on the operating system (OS) in use.</p> Ubuntu/DebianCentOS/RHEL Bash Session<pre><code>$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> Bash Session<pre><code>  $ yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#what-is-a-dockerfile","title":"What is a Dockerfile?","text":"<p>Before we discuss what is a Dockerfile, it is important to know what a Docker image is. Docker Image:</p> <p>A Docker Image is a read-only file with a bunch of instructions. When these instructions are executed, it creates a Docker container. Dockerfile:</p> <p>Dockerfile is a simple text file that consists of instructions to build Docker images.</p> <p>Mentioned below is the syntax of a Dockerfile:</p> SyntaxExample Docker<pre><code># comments\n\ncommand argument argument1...\n</code></pre> Docker<pre><code># \"This line commented\"\n\nRun echo \"Get Certified. Get Ahead\"\n</code></pre> <p>Now, let's have a look at how to build a Docker image using a dockerfile.</p>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#dockerfile-commands","title":"Dockerfile Commands","text":"<p>Below is a list of Docker commands that can be used to create a Dockerfile, which is a text file used to build Docker images:</p> <ol> <li>FROM: Specifies the base image to be used as the starting point for the Docker image.</li> <li>RUN: Executes commands in the Docker image during the build process.</li> <li>COPY: Copies files and directories from the host machine to the Docker image.</li> <li>ADD: Similar to COPY, but with additional support for URLs and unpacking compressed files.</li> <li>ENV: Sets environment variables within the Docker image.</li> <li>WORKDIR: Sets the working directory for subsequent commands in the Docker image.</li> <li>EXPOSE: Informs Docker that the container listens on specific network ports at runtime.</li> <li>CMD: Specifies the default command to run when the Docker container is started.</li> <li>ENTRYPOINT: Configures the container to run as an executable and defines the command that will be executed.</li> <li>VOLUME: Creates a mount point for a volume or a shared directory.</li> </ol> <p>These Docker commands can be used in combination to define the steps required to build a Docker image that encapsulates your application or service.</p> <ol> <li> <p>FROM Command: The FROM command specifies the base image to be used as the starting point for the Docker image. It defines the environment and dependencies required by your application. For example:</p> Docker<pre><code>FROM ubuntu:20.04\n</code></pre> </li> <li> <p>RUN Command: The RUN command executes commands in the Docker image during the build process. It is used to install packages, update software, and run any necessary setup steps. Here's an example:</p> Docker<pre><code>RUN apt-get update &amp;&amp; apt-get install -y curl\n</code></pre> </li> <li> <p>COPY Command: The COPY command copies files and directories from the host machine to the Docker image. It is useful for adding application code, configuration files, or any other required files. An example usage is as follows:</p> Docker<pre><code>COPY app.py /app/\n</code></pre> </li> <li> <p>ADD Command: The ADD command is similar to COPY but provides additional features. It can handle URLs and automatically unpack compressed files. Here's an example:</p> Docker<pre><code>ADD https://example.com/archive.tar.gz /tmp/\n</code></pre> </li> <li> <p>ENV Command: The ENV command sets environment variables within the Docker image. It allows you to provide configuration values to your application. For instance:     Docker<pre><code>ENV PORT 8080\nENV DB_HOST localhost\n</code></pre></p> </li> <li> <p>WORKDIR Command: The WORKDIR command sets the working directory for subsequent commands in the Docker image. It simplifies the path references in subsequent commands. An example usage is as follows:</p> Docker<pre><code>WORKDIR /app\n</code></pre> </li> <li> <p>EXPOSE Command: The EXPOSE command informs Docker that the container listens on specific network ports at runtime. It does not actually publish the ports, but serves as documentation for container users. An example usage is as follows:</p> Docker<pre><code>EXPOSE 8080\n</code></pre> </li> <li> <p>CMD Command: The CMD command specifies the default command to run when the Docker container is started. It defines the main executable or script for the container. Here's an example:</p> Docker<pre><code>CMD [\"python\", \"app.py\"]\n</code></pre> </li> <li> <p>ENTRYPOINT Command: The ENTRYPOINT command configures the container to run as an executable and defines the command that will be executed. It is typically used in conjunction with the CMD command. An example usage is as follows:</p> Docker<pre><code>ENTRYPOINT [\"python\", \"app.py\"]\n</code></pre> </li> <li> <p>VOLUME Command: The VOLUME command creates a mount point for a volume or a shared directory. It allows data to persist beyond the container's lifespan. Here's an example:</p> Docker<pre><code>VOLUME /data\n</code></pre> </li> </ol>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"devops/step_by_step_guide_about_docker_image_build/#docker-image-build","title":"Docker Image Build","text":"<ol> <li> <p>Step 1: Create a Dockerfile</p> <p>Open a text editor and create a new file named \"Dockerfile\" (without any file extension). Add the necessary instructions and configurations to the Dockerfile, such as the base image, required dependencies, and application setup.</p> <p>Example Dockerfile</p> Docker<pre><code>FROM ubuntu\n\nMAINTAINER simple\n\nRUN apt-get update\n\nCMD [\"echo\", \"Welcome to Simple-learn\"]\n</code></pre> </li> <li> <p>Step 2: Build the Docker Image</p> <ol> <li>Open a terminal or command prompt and navigate to the directory containing the Dockerfile.</li> <li> <p>Run the following command to build the Docker image:</p> Bash Session<pre><code>docker image build -t myapp:latest .\n</code></pre> <ul> <li><code>-t</code> specifies the image tag, which is set to \"myapp:latest\" in this example.</li> <li><code>.</code> specifies the build context, which is the current directory.</li> </ul> </li> </ol> </li> <li> <p>Step 3: Monitor the Build Process</p> <ol> <li>Docker will start building the image and display the progress and status of each step defined in the Dockerfile.</li> <li>Docker will also download any required base images or dependencies specified in the Dockerfile.</li> </ol> </li> <li> <p>Step 4: Verify the Built Image</p> <p>Once the build process completes successfully, you can verify the newly built image using the following command:</p> <p>Bash Session<pre><code>docker image ls\n</code></pre> This command lists all the available Docker images on your system. Ensure that the image \"myapp\" is listed with the appropriate tag and size.</p> </li> <li> <p>Step 5: Run the Docker Image as a Container</p> </li> </ol> <p>In the terminal or command prompt, execute the following command to run the Docker image as a container:</p> <p>Bash Session<pre><code>docker run [OPTIONS] IMAGE[:TAG] [COMMAND] [ARG...]\n</code></pre>   - Replace [OPTIONS] with any desired runtime options, such as specifying ports, environment variables, or volumes.   - Replace IMAGE[:TAG] with the image ID or repository:tag obtained from Step 1.   - Replace [COMMAND] [ARG...] with any desired command and arguments to be executed inside the container.</p> <p>Example</p> <p>Let's consider an example where we have a Docker image named \"myapp:latest\" that simply prints message.</p> Bash Session<pre><code>docker run -d  --name myapp-container myapp:latest\n</code></pre> <p>The message 'Welcome to Simplelearn' should appear in the command line, as seen in the image above.</p>","tags":["Docker","Dockerfile","docker tutorial","docker build","installation of docker","DevOps"]},{"location":"kubernetes/","title":"Kubernetes","text":"<p>we unlock the transformative power of Kubernetes, taking your business to unparalleled heights of efficiency, scalability, and innovation. Like a virtuoso conductor, Kubernetes orchestrates the symphony of your applications, harmonizing their performance and ensuring flawless execution across any infrastructure.</p> <p>Embark on a journey of digital empowerment as Kubernetes reigns supreme as the crown jewel of container orchestration. Seamlessly navigating the complexities of modern technology, it creates a dynamic ecosystem that propels your organization towards accelerated development cycles, increased resilience, and unprecedented cost savings.</p> <p>Step into a world where containers become second nature, floating effortlessly across clouds, servers, and clusters. Kubernetes, the celestial navigator, eliminates the shackles of manual management, liberating your teams to focus on what truly matters \u2013 innovation and value creation.</p> <p>Our holistic approach to Kubernetes empowers you with the ability to effortlessly scale your applications, adapt to surges in demand, and gracefully handle unexpected challenges. Embrace the elasticity and agility of this revolutionary platform, where your digital ambitions reach new heights and boundaries cease to exist.</p> <p>With Kubernetes as your guiding force, embrace a future defined by rapid deployment, seamless scalability, and fault-tolerant systems. Harness the power of containers and unlock unparalleled productivity, enabling your organization to transcend limitations and redefine what's possible.</p> <p>Join us on this transformative journey and discover the untapped potential of Kubernetes \u2013 where the magic of orchestration intertwines with the art of possibility, propelling your business towards a brighter, more prosperous tomorrow.</p>","tags":["Kubernetes","Learn","DevOps","k8s","cluster"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/","title":"Introduction About Kubernetes","text":"<p>Kubernetes is also known as 'k8s'. Kubernetes is an extensible, portable, and open-source container management platform designed by Google. Kubernetes helps to manage containerised applications in various types of physical, virtual, and cloud environments.</p> <p>Kubernetes helps you to control the resource allocation and traffic management for cloud applications and microservices. It also helps to simplify various aspects of service-oriented infrastructures. Kubernetes allows you to assure where and when containerized applications run and helps you to find resources and tools you want to work with.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#kubernetes-features","title":"Kubernetes Features","text":"<ul> <li>Automated Scheduling</li> <li>Self-Healing Capabilities</li> <li>Automated rollouts &amp; rollback</li> <li>Horizontal Scaling &amp; Load Balancing</li> <li>Offers environment consistency for development, testing, and     production</li> <li>Infrastructure is loosely coupled to each component can act as a     separate unit</li> <li>Provides a higher density of resource utilization</li> <li>Offers enterprise-ready features</li> <li>Application-centric management</li> <li>Auto-scalable infrastructure</li> <li>You can create predictable infrastructure</li> </ul>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#kubernetes-architecture","title":"Kubernetes Architecture","text":"","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#master-node","title":"Master Node","text":"<p>The master node is the first and most vital component which is responsible for the management of Kubernetes cluster. It is the entry point for all kind of administrative tasks. There might be more than one master node in the cluster to check for fault tolerance.</p> <p>The master node has various components like API Server, Controller Manager, Scheduler, and ETCD. Let see all of them.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#api-server","title":"API Server:","text":"<p>It provides all the operation on cluster using the API. API server implements an interface, which means different tools and libraries can readily communicate with it. Kubeconfig is a package along with the server side tools that can be used for communication.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#controller-manager","title":"Controller Manager","text":"<p>This component is responsible for most of the collectors that regulates the state of cluster and performs a task. In general, it can be considered as a daemon which runs in nonterminating loop and is responsible for collecting and sending information to API server. It works toward getting the shared state of cluster and then make changes to bring the current status of the server to the desired state. The key controllers are replication controller, endpoint controller, namespace controller, and service account controller. The controller manager runs different kind of controllers to handle nodes, endpoints, etc.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#scheduler","title":"Scheduler","text":"<p>This is one of the key components of Kubernetes master. It is a service in master responsible for distributing the workload. It is responsible for tracking utilization of working load on cluster nodes and then placing the workload on which resources are available and accept the workload. In other words, this is the mechanism responsible for allocating pods to available nodes. The scheduler is responsible for workload utilization and allocating pod to new node.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#etcd","title":"Etcd","text":"<p>It stores the configuration information which can be used by each of the nodes in the cluster. It is a high availability key value store that can be distributed among multiple nodes. It is accessible only by Kubernetes API server as it may have some sensitive information. It is a distributed key value Store which is accessible to all.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#workerslave-nodes","title":"Worker/Slave nodes","text":"<p>Worker nodes are another essential component which contains all the required services to manage the networking between the containers, communicate with the master node, which allows you to assign resources to the scheduled containers.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#docker","title":"Docker","text":"<p>The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#kubelet-service","title":"Kubelet Service","text":"<p>This is a small service in each node responsible for relaying information to and from control plane service. It interacts with etcd store to read configuration details and wright values. This communicates with the master component to receive commands and work. The kubelet process then assumes responsibility for maintaining the state of work and the node server. It manages network rules, port forwarding, etc.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#kubernetes-proxy-service","title":"Kubernetes Proxy Service","text":"<p>This is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to correct containers and is capable of performing primitive load balancing. It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. It manages pods on node, volumes, secrets, creating new containers' health checkup, etc.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#kubernetes-key-terminologies","title":"Kubernetes Key Terminologies","text":"","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#cluster","title":"Cluster:","text":"<p>It is a collection of hosts(servers) that helps you to aggregate their available resources. That includes ram, CPU, ram, disk, and their devices into a usable pool.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#master","title":"Master:","text":"<p>The master is a collection of components which make up the control panel of Kubernetes. These components are used for all cluster decisions. It includes both scheduling and responding to cluster events.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#node","title":"Node:","text":"<p>A node is a working machine in Kubernetes cluster . They are working units which can be physical, VM, or a cloud instance.Each node has all the required configuration required to run a pod on it such as the proxy service and kubelet service along with the Docker, which is used to run the Docker containers on the pod created on the node.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#namespace","title":"Namespace:","text":"<p>It is a logical cluster or environment. It is a widely used method which is used for scoping access or dividing a cluster. It provides an additional qualification to a resource name. This is helpful when multiple teams are using the same cluster and there is a potential of name collision. It can be as a virtual wall between multiple clusters.</p> <p>Following are some of the important functionalities of a Namespace in Kubernetes</p> <ul> <li>Namespaces help pod-to-pod communication using the same namespace.</li> <li>Namespaces are virtual clusters that can sit on top of the same     physical cluster.</li> <li>They provide logical separation between the teams and their     environments.</li> </ul> YAML<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  namespace: elk\n  labels:\n      component: elasticsearch\nspec:\n  type: LoadBalancer\n  selector:\n      component: elasticsearch\n  ports:\n  - name: http\n      port: 9200\n      protocol: TCP\n  - name: transport\n      port: 9300\n      protocol: TCP\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#labels-and-selectors","title":"Labels and Selectors:","text":"<p>Labels are key-value pairs which are attached to pods, replication controller and services. They are used as identifying attributes for objects such as pods and replication controller. They can be added to an object at creation time and can be added or modified at the run time.</p> <p>Unlike names and UIDs, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).</p> <p>A label selector is just a fancy name of the mechanism that enables the client/user to target (select) a set of objects by their labels.</p> <p>It can be confusing because different resource types support different selector types - 'selector' vs 'matchExpressions' vs 'matchLabels':</p> <p></p> <p>Newer resource types like Deployment, Job, DaemonSet, and ReplicaSet support both 'matchExpressions' and 'matchLabels', but only one of them can be nested under the 'selector' section, while the other resources (like \"Service\" in the example above) support only 'matchLabels', so there is no need to define which option is used, because only one option is available for those resource types.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#service","title":"Service","text":"<p>A service can be defined as a logical set of pods. It can be defined as an abstraction on the top of the pod which provides a single IP address and DNS name by which pods can be accessed. With Service, it is very easy to manage load balancing configuration. It helps pods to scale very easily.</p> <p>A service is a REST object in Kubernetes whose definition can be posted to Kubernetes apiServer on the Kubernetes master to create a new instance.</p> YAML<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre> <p>Types of Services are:</p> <ul> <li>ClusterIP</li> <li>NodePort</li> <li>LoadBalancer</li> <li>ExternalName</li> </ul>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#clusterip","title":"ClusterIP","text":"<p>An internal fixed IP known as a ClusterIP can be created in front of a pod or a replica as necessary.</p> <p>The ClusterIP provides a load-balanced IP address. One or more pods that match a label selector can forward traffic to the IP address. The ClusterIP service must define one or more ports to listen on with target ports to forward TCP/UDP traffic to containers. The IP address that is used for the ClusterIP is not routable outside of the cluster, like the pod IP address is.</p> <p></p> YAML<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-clusterip-service\nspec:\n  selector:\n    app: nginx\n  type: ClusterIP\n  ports:\n    - protocol: TCP\n      port: 80\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#nodeport","title":"NodePort","text":"<p>Services of type NodePort build on top of ClusterIP type services by exposing the ClusterIP service outside of the cluster on high ports (default 30000-32767). If no port number is specified then Kubernetes automatically selects a free port. The local kube-proxy is responsible for listening to the port on the node and forwarding client traffic on the NodePort to the ClusterIP.</p> <p></p> <p>Users can communicate with the service from the outside by requesting \\&lt;NodeIP&gt;:\\&lt;NodePort&gt;</p> <p>By default, every node in the cluster listens on this port, including nodes where the pod that matches the label selector does not run. Traffic on such nodes is internally NATed and forwarded to the target pod (Cluster external traffic policy).</p> YAML<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-nodeport-service\nspec:\n  selector:\n    app: nginx\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#load-balancer","title":"Load Balancer","text":"<p>The LoadBalancer service type is built on top of NodePort service types by provisioning and configuring external load balancers from public and private cloud providers. It exposes services that are running in the cluster by forwarding layer 4 traffic to worker nodes. This is a dynamic way of implementing a case that involves external load balancers and NodePort type services. However, it usually requires an integration that runs inside the Kubernetes cluster that performs a watch on the API for services of type LoadBalancer.</p> <p></p> <p>It will also automatically create ClusterIP and NodePort services and route traffic accordingly.</p> YAML<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-loadbalancer-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n      nodePort: 31999\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#externalname","title":"ExternalName","text":"<p>This type of service maps the service to the contents of the externalName field (Ex: app.test.com). It does this by returning a value for the CNAME record.</p>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#pod","title":"Pod","text":"<p>A pod is a collection of containers and its storage inside a node of a Kubernetes cluster. It is possible to create a pod with multiple containers inside it. For example, keeping a database container and data container in the same pod.</p> <p>There are two types of Pods</p> <ul> <li>Single container pod</li> <li>Multi container pod</li> </ul>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#single-container-pod","title":"Single Container Pod","text":"<p>If you defined single image on Pod Specification then it would created with single container.</p> YAML<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: Nginx\n    image: nginx\n    ports:\ncontainerPort: 7500\n  imagePullPolicy: Always\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#multi-container-pod","title":"Multi Container Pod","text":"<p>If you defined multiple image on Pod Specification then it would created with multi container with each image specified.</p> YAML<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: Nginx\n    image: nginx\n    ports:\n      containerPort: 7500\n    imagePullPolicy: Always\n  - name: Database\n    Image: mongoDB\n    Ports:\n      containerPort: 7501\n    imagePullPolicy: Always\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#deployments","title":"Deployments","text":"<p>Deployments are upgraded and higher version of replication controller and additional feature from ReplicaSet. They manage the deployment of replica sets which is also an upgraded version of the replication controller. They have the capability to update the replica set and are also capable of rolling back to the previous version.</p> <p>They provide many updated features of matchLabels and selectors. We have got a new controller in the Kubernetes master called the deployment controller which makes it happen. It has the capability to change the deployment midway.</p> YAML<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n          - name: http\n            containerPort: 80\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/introduction_about_kubernetes_and_basics/#secrets","title":"Secrets","text":"<p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code.</p> <p>Because Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, such as avoiding writing secret data to nonvolatile storage.</p> <p>Secrets are similar to ConfigMaps but are specifically intended to hold confidential data.</p> <p>There are three main ways of uses for Secrets:</p> <ul> <li>As files in a volume mounted on one or more of its containers.</li> <li>As container environment variable.</li> <li>By the kubelet when pulling images for the Pod.</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  username: admin\n  password: Passw0rd0\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\n</code></pre> <p>To use a Secret in an environment variable in a Pod:</p> <ul> <li>Create a Secret (or use an existing one). Multiple Pods can     reference the same Secret.</li> <li>Modify your Pod definition in each container that you wish to     consume the value of a secret key to add an environment variable for     each secret key you wish to consume. The environment variable that     consumes the secret key should populate the secret's name and key     in env[].valueFrom.secretKeyRef.</li> <li>Modify your image and/or command line so that the program looks for     values in the specified environment variables.</li> </ul> YAML<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-env-pod\nspec:\n  containers:\n  - name: mycontainer\n    image: redis\n    env:\n      - name: SECRET_USERNAME\n        valueFrom:\n          secretKeyRef:\n            name: mysecret\n            key: username\n            optional: false # same as default; \"mysecret\" must exist\n                            # and include a key named \"username\"\n      - name: SECRET_PASSWORD\n        valueFrom:\n          secretKeyRef:\n            name: mysecret\n            key: password\n            optional: false # same as default; \"mysecret\" must exist\n                            # and include a key named \"password\"\n  restartPolicy: Never\n</code></pre> <p>Create a Secret from File , run the below command to do the same. However the adjust the file location according to your file location available.</p> Bash Session<pre><code>kubectl create secret mysecret-from-file db-user-pass --from-file=./username.txt --from-file=./password.txt\n</code></pre> <p>To use a Secret as a File in a Pod:</p> <p>If you want to access data from a Secret in a Pod, one way to do that is to have Kubernetes make the value of that Secret be available as a file inside the filesystem of one or more of the Pod's containers.</p> <ul> <li>Create a secret or use an existing one. Multiple Pods can reference     the same secret.</li> <li>Modify your Pod definition to add a volume under .spec.volumes[].     Name the volume anything, and have a     .spec.volumes[].secret.secretName field equal to the name of the     Secret object.</li> <li>Add a .spec.containers[].volumeMounts[] to each container that     needs the secret. Specify     .spec.containers[].volumeMounts[].readOnly = true and     .spec.containers[].volumeMounts[].mountPath to an unused     directory name where you would like the secrets to appear.</li> <li>Modify your image or command line so that the program looks for     files in that directory. Each key in the secret data map becomes the     filename under mountPath.</li> </ul> YAML<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: username.txt\n      mountPath: \"/etc/username.txt\"\n      readOnly: true\n    - name: password\n      mountPath: \"/etc/password.txt\"\n      readOnly: true\n  volumes:\n  - name: username.txt\n    secret:\n      secretName: mysecret-from-file\n      optional: false # default setting; \"mysecret\" must exist\n  - name: password.txt\n    secret:\n      secretName: mysecret-from-file\n      optional: false # default setting; \"mysecret\" must exist\n</code></pre>","tags":["Kubernetes","DevOps","Kubernetes Basics","Kubernetes Tutorial"]},{"location":"kubernetes/propagate_sigterm_to_k8s_pod/","title":"Propagating a SIGTERM Signal to the Main Process in a Kubernetes Pod","text":"","tags":["kubernetes","SIGTERM Handling","Pod Termination","Termination Grace Period"]},{"location":"kubernetes/propagate_sigterm_to_k8s_pod/#introduction","title":"Introduction","text":"<p>As a Kubernetes user, you may find that when you restart a pod in Kubernetes, the process running inside the container does not receive the SIGTERM signal. Instead, the signal is only sent to the PID of the entrypoint shell script.</p> <p>To understand why this is happening, it\u2019s important to know that Kubernetes sends a SIGTERM signal to a container\u2019s main process when the container is terminated or restarted. However, if the entrypoint shell script is not properly propagating the SIGTERM signal to the process it launches, then only the entrypoint shell script will receive the signal.</p> <p>So how can you ensure that the SIGTERM signal is properly propagated to the main process in a Kubernetes pod? In this article, we\u2019ll walk through an example entrypoint script that demonstrates how to propagate the SIGTERM signal to the main process.</p>","tags":["kubernetes","SIGTERM Handling","Pod Termination","Termination Grace Period"]},{"location":"kubernetes/propagate_sigterm_to_k8s_pod/#understanding-the-sigterm-signal","title":"Understanding the SIGTERM Signal","text":"<p>Before we dive into the entrypoint script, let\u2019s first take a closer look at the SIGTERM signal. SIGTERM is a signal that is sent to a process to request that it gracefully terminate. By gracefully terminating, the process has the opportunity to perform any necessary cleanup tasks before exiting.</p> <p>In Kubernetes, when a pod is terminated or restarted, Kubernetes sends a SIGTERM signal to the main process of each container in the pod. By default, the main process will receive the signal and have the opportunity to perform any necessary cleanup tasks before exiting.</p> <p>However, if the main process launches additional child processes, it\u2019s important to ensure that the SIGTERM signal is propagated to those child processes as well. Otherwise, those child processes may be abruptly terminated and leave behind unclean state.</p>","tags":["kubernetes","SIGTERM Handling","Pod Termination","Termination Grace Period"]},{"location":"kubernetes/propagate_sigterm_to_k8s_pod/#propagating-the-sigterm-signal","title":"Propagating the SIGTERM Signal","text":"<p>To properly propagate the SIGTERM signal to the main process in a Kubernetes pod, you need to modify the entrypoint script to ensure that the signal is forwarded to the main process.</p> <p>One way to do this is to use the exec command to replace the shell script process with the main process when launching it. This way, when the SIGTERM signal is sent to the shell script process, it will be forwarded to the main process.</p> <p>Here\u2019s an example entrypoint script that demonstrates how to propagate the SIGTERM signal to the main process:</p> Bash<pre><code>#!/bin/bash\n\n# Start the main process and save its PID\n# Use exec to replace the shell script process with the main process\nexec my_main_process &amp;\npid=$!\n\n# Trap the SIGTERM signal and forward it to the main process\ntrap 'kill -SIGTERM $pid; wait $pid' SIGTERM\n\n# Wait for the main process to complete\nwait $pid\n</code></pre> <p>In the above example, the <code>exec</code> command is used to launch <code>my_main_process</code> and replace the shell script process with it. The <code>pid</code> variable is used to store the PID of the main process. The <code>trap</code> command is used to catch the SIGTERM signal and forward it to the main process using the <code>kill</code> command. Finally, the <code>wait</code> command is used to wait for the main process to complete.</p> <p>By using this technique, you can ensure that the SIGTERM signal is properly propagated to the main process when restarting the pod.</p>","tags":["kubernetes","SIGTERM Handling","Pod Termination","Termination Grace Period"]},{"location":"kubernetes/propagate_sigterm_to_k8s_pod/#conclusion","title":"Conclusion","text":"<p>In summary, when you restart a pod in Kubernetes, it\u2019s important to ensure that the SIGTERM signal is properly propagated to the main process. By modifying your entrypoint script to use the exec command and trap the SIGTERM signal, you can ensure that the signal is forwarded to the main process and that it has the opportunity to gracefully terminate.</p>","tags":["kubernetes","SIGTERM Handling","Pod Termination","Termination Grace Period"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/","title":"Redis-Cluster Deployment on Kubernetes","text":"","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/#configmap-configuration","title":"ConfigMap - Configuration","text":"<p>Add below content on configmap.yaml and apply on kubernetes using <code>kubectl apply -f configmap.yaml</code></p> Text Only<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-cluster-default\n  labels:\n    app.kubernetes.io/name: redis-cluster\n    helm.sh/chart: redis-cluster-7.4.6\n    app.kubernetes.io/instance: redis-cluster\n    app.kubernetes.io/managed-by: Helm\ndata:\n  redis-default.conf: |-\n    bind 127.0.0.1\n    protected-mode yes\n    port 6379\n    tcp-backlog 511\n    timeout 0\n    tcp-keepalive 300\n    daemonize no\n    supervised no\n    pidfile /opt/bitnami/redis/tmp/redis_6379.pid\n    loglevel notice\n    logfile \"\"\n    databases 16\n    always-show-logo yes\n    save 900 1\n    save 300 10\n    save 60 10000\n    stop-writes-on-bgsave-error yes\n    rdbcompression yes\n    rdbchecksum yes\n    dbfilename dump.rdb\n    rdb-del-sync-files no\n    dir /bitnami/redis/data\n    replica-serve-stale-data yes\n    replica-read-only yes\n    repl-diskless-sync no\n    repl-diskless-sync-delay 5\n    repl-diskless-load disabled\n    repl-disable-tcp-nodelay no\n    replica-priority 100\n    acllog-max-len 128\n    lazyfree-lazy-eviction no\n    lazyfree-lazy-expire no\n    lazyfree-lazy-server-del no\n    replica-lazy-flush no\n    lazyfree-lazy-user-del no\n    appendonly no\n    appendfilename \"appendonly.aof\"\n    appendfsync everysec\n    no-appendfsync-on-rewrite no\n    auto-aof-rewrite-percentage 100\n    auto-aof-rewrite-min-size 64mb\n    aof-load-truncated yes\n    aof-use-rdb-preamble yes\n    lua-time-limit 5000\n    cluster-enabled yes\n    cluster-config-file /bitnami/redis/data/nodes.conf\n    slowlog-log-slower-than 10000\n    slowlog-max-len 128\n    latency-monitor-threshold 0\n    notify-keyspace-events \"\"\n    hash-max-ziplist-entries 512\n    hash-max-ziplist-value 64\n    list-max-ziplist-size -2\n    list-compress-depth 0\n    set-max-intset-entries 512\n    zset-max-ziplist-entries 128\n    zset-max-ziplist-value 64\n    hll-sparse-max-bytes 3000\n    stream-node-max-bytes 4096\n    stream-node-max-entries 100\n    activerehashing yes\n    client-output-buffer-limit normal 0 0 0\n    client-output-buffer-limit replica 256mb 64mb 60\n    client-output-buffer-limit pubsub 32mb 8mb 60\n    hz 10\n    dynamic-hz yes\n    aof-rewrite-incremental-fsync yes\n    rdb-save-incremental-fsync yes\n    jemalloc-bg-thread yes\n</code></pre>","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/#configmap-scripts","title":"ConfigMap - Scripts","text":"<p>Add below content on scripts-configmap.yaml and apply on kubernetes using <code>kubectl apply -f scripts-configmap.yaml</code></p> Text Only<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis-cluster-scripts\n  labels:\n    app.kubernetes.io/name: redis-cluster\n    helm.sh/chart: redis-cluster-7.4.6\n    app.kubernetes.io/instance: redis-cluster\n    app.kubernetes.io/managed-by: Helm\ndata:\n  ping_readiness_local.sh: |-\n    #!/bin/sh\n    set -e\n\n    REDIS_STATUS_FILE=/tmp/.redis_cluster_check\n    if [ ! -z \"$REDIS_PASSWORD\" ]; then export REDISCLI_AUTH=$REDIS_PASSWORD; fi;\n    response=$(\n      timeout -s 3 $1 \\\n      redis-cli \\\n        -h localhost \\\n        -p $REDIS_PORT \\\n        ping\n    )\n    if [ \"$?\" -eq \"124\" ]; then\n      echo \"Timed out\"\n      exit 1\n    fi\n    if [ \"$response\" != \"PONG\" ]; then\n      echo \"$response\"\n      exit 1\n    fi\n    if [ ! -f \"$REDIS_STATUS_FILE\" ]; then\n      response=$(\n        timeout -s 3 $1 \\\n        redis-cli \\\n          -h localhost \\\n          -p $REDIS_PORT \\\n          CLUSTER INFO | grep cluster_state | tr -d '[:space:]'\n      )\n      if [ \"$?\" -eq \"124\" ]; then\n        echo \"Timed out\"\n        exit 1\n      fi\n      if [ \"$response\" != \"cluster_state:ok\" ]; then\n        echo \"$response\"\n        exit 1\n      else\n        touch \"$REDIS_STATUS_FILE\"\n      fi\n    fi\n  ping_liveness_local.sh: |-\n    #!/bin/sh\n    set -e\n    if [ ! -z \"$REDIS_PASSWORD\" ]; then export REDISCLI_AUTH=$REDIS_PASSWORD; fi;\n    response=$(\n      timeout -s 3 $1 \\\n      redis-cli \\\n        -h localhost \\\n        -p $REDIS_PORT \\\n        ping\n    )\n    if [ \"$?\" -eq \"124\" ]; then\n      echo \"Timed out\"\n      exit 1\n    fi\n    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')\n    if [ \"$response\" != \"PONG\" ] &amp;&amp; [ \"$responseFirstWord\" != \"LOADING\" ] &amp;&amp; [ \"$responseFirstWord\" != \"MASTERDOWN\" ]; then\n      echo \"$response\"\n      exit 1\n    fi\n</code></pre>","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/#headless-service","title":"Headless Service","text":"Text Only<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-cluster-headless\n  labels:\n    app.kubernetes.io/name: redis-cluster\n    helm.sh/chart: redis-cluster-7.4.6\n    app.kubernetes.io/instance: redis-cluster\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n    - name: tcp-redis\n      port: 6379\n      targetPort: tcp-redis\n    - name: tcp-redis-bus\n      port: 16379\n      targetPort: tcp-redis-bus\n  selector:\n    app.kubernetes.io/name: redis-cluster\n    app.kubernetes.io/instance: redis-cluster\n</code></pre>","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/#secret-store","title":"Secret Store","text":"Text Only<pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: redis-cluster\n  labels:\n    app.kubernetes.io/name: redis-cluster\n    helm.sh/chart: redis-cluster-7.4.6\n    app.kubernetes.io/instance: redis-cluster\n    app.kubernetes.io/managed-by: Helm\ntype: Opaque\ndata:\n  redis-password: \"Q3lpc3c1Q3Y0TQ==\"\n</code></pre>","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/#redis-service","title":"Redis Service","text":"Text Only<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-cluster\n  labels:\n    app.kubernetes.io/name: redis-cluster\n    helm.sh/chart: redis-cluster-7.4.6\n    app.kubernetes.io/instance: redis-cluster\n    app.kubernetes.io/managed-by: Helm\n  annotations:\nspec:\n  type: ClusterIP\n  ports:\n    - name: tcp-redis\n      port: 6379\n      targetPort: tcp-redis\n      protocol: TCP\n      nodePort: null\n  selector:\n    app.kubernetes.io/name: redis-cluster\n    app.kubernetes.io/instance: redis-cluster\n</code></pre>","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"kubernetes/redis-cluster_deployment_on_kubernetes/#statefulset-deployment","title":"StatefulSet Deployment","text":"Text Only<pre><code>---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-cluster\n  labels:\n    app.kubernetes.io/name: redis-cluster\n    helm.sh/chart: redis-cluster-7.4.6\n    app.kubernetes.io/instance: redis-cluster\n    app.kubernetes.io/managed-by: Helm\nspec:\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: redis-cluster\n      app.kubernetes.io/instance: redis-cluster\n  replicas: 2\n  serviceName: redis-cluster-headless\n  podManagementPolicy: Parallel\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: redis-cluster\n        helm.sh/chart: redis-cluster-7.4.6\n        app.kubernetes.io/instance: redis-cluster\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      enableServiceLinks: false\n\n      securityContext:\n        fsGroup: 1001\n        runAsUser: 1001\n        sysctls: []\n      serviceAccountName: default\n      affinity:\n        podAffinity:\n\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - podAffinityTerm:\n                labelSelector:\n                  matchLabels:\n                    app.kubernetes.io/name: redis-cluster\n                    app.kubernetes.io/instance: redis-cluster\n                topologyKey: kubernetes.io/hostname\n              weight: 1\n        nodeAffinity:\n\n      containers:\n        - name: redis-cluster\n          image: docker.io/bitnami/redis-cluster:6.2.6-debian-10-r178\n          imagePullPolicy: \"IfNotPresent\"\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1001\n          command: ['/bin/bash', '-c']\n          args:\n            - |\n              # Backwards compatibility change\n              if ! [[ -f /opt/bitnami/redis/etc/redis.conf ]]; then\n                  echo COPYING FILE\n                  cp  /opt/bitnami/redis/etc/redis-default.conf /opt/bitnami/redis/etc/redis.conf\n              fi\n              pod_index=($(echo \"$POD_NAME\" | tr \"-\" \"\\n\"))\n              pod_index=\"${pod_index[-1]}\"\n              if [[ \"$pod_index\" == \"0\" ]]; then\n                export REDIS_CLUSTER_CREATOR=\"yes\"\n                export REDIS_CLUSTER_REPLICAS=\"1\"\n              fi\n              /opt/bitnami/scripts/redis-cluster/entrypoint.sh /opt/bitnami/scripts/redis-cluster/run.sh\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: REDIS_NODES\n              value: \"redis-cluster-0.redis-cluster-headless redis-cluster-1.redis-cluster-headless \"\n            - name: REDISCLI_AUTH\n              valueFrom:\n                secretKeyRef:\n                  name: redis-cluster\n                  key: redis-password\n            - name: REDIS_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: redis-cluster\n                  key: redis-password\n            - name: REDIS_AOF_ENABLED\n              value: \"yes\"\n            - name: REDIS_TLS_ENABLED\n              value: \"no\"\n            - name: REDIS_PORT\n              value: \"6379\"\n          ports:\n            - name: tcp-redis\n              containerPort: 6379\n            - name: tcp-redis-bus\n              containerPort: 16379\n          livenessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            # One second longer than command timeout should prevent generation of zombie processes.\n            timeoutSeconds: 6\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /scripts/ping_liveness_local.sh 5\n          readinessProbe:\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            # One second longer than command timeout should prevent generation of zombie processes.\n            timeoutSeconds: 2\n            successThreshold: 1\n            failureThreshold: 5\n            exec:\n              command:\n                - sh\n                - -c\n                - /scripts/ping_readiness_local.sh 1\n          resources:\n            limits: {}\n            requests: {}\n          volumeMounts:\n            - name: scripts\n              mountPath: /scripts\n            - name: redis-data\n              mountPath: /bitnami/redis/data\n              subPath:\n            - name: default-config\n              mountPath: /opt/bitnami/redis/etc/redis-default.conf\n              subPath: redis-default.conf\n            - name: redis-tmp-conf\n              mountPath: /opt/bitnami/redis/etc/\n      volumes:\n        - name: scripts\n          configMap:\n            name: redis-cluster-scripts\n            defaultMode: 0755\n        - name: default-config\n          configMap:\n            name: redis-cluster-default\n        - name: redis-tmp-conf\n          emptyDir: {}\n  volumeClaimTemplates:\n    - metadata:\n        name: redis-data\n        labels:\n          app.kubernetes.io/name: redis-cluster\n          app.kubernetes.io/instance: redis-cluster\n      spec:\n        accessModes:\n          - \"ReadWriteOnce\"\n        resources:\n          requests:\n            storage: \"8Gi\"\n</code></pre>","tags":["Kubernetes","Cluster","Redis","Redis Cluster"]},{"location":"linux/","title":"Linux","text":"<p>we celebrate the incredible power and versatility of Linux, the ultimate operating system for those seeking unparalleled freedom and control over their digital experience.</p> <p>Linux, the revolutionary brainchild of visionary developers, is an open-source marvel that breathes life into computers, servers, and devices around the world. Embodying the spirit of collaboration and innovation, Linux stands as a shining symbol of the endless possibilities that emerge when like-minded individuals come together to build something extraordinary.</p> <p>Immerse yourself in the world of Linux, where boundaries are shattered, and limitations are mere illusions. With Linux, every user becomes a pioneer, equipped with an arsenal of customizable tools and a robust foundation that empowers them to create, explore, and push the boundaries of what is possible.</p> <p>Step into a realm where creativity thrives and possibilities abound. Linux embraces diversity, providing a home to a multitude of distributions, each with its own unique flavor, tailored to cater to the needs and preferences of its users. Whether you seek a sleek and minimalist interface or a feature-rich powerhouse, Linux has the perfect distribution to elevate your digital journey.</p> <p>Discover the unparalleled security and stability of Linux, safeguarding your digital sanctuary from threats and instilling peace of mind. Harness the power of an operating system that empowers you to take charge of your privacy and data, ensuring that your digital footprint remains firmly under your control.</p> <p>At the core of Linux lies the spirit of community, a vibrant ecosystem of passionate individuals who continuously contribute their expertise, driving the evolution of this remarkable operating system. As you navigate through our website, you'll witness firsthand the unwavering dedication and collective brilliance that fuels the Linux community.</p> <p>Join us on this extraordinary adventure as we invite you to explore the limitless possibilities of Linux. Unleash your creativity, embrace innovation, and embark on a journey where technology bends to your will. Welcome to the world of Linux, where freedom, power, and endless potential are yours to embrace.</p> <p>Linux is a UNIX-based open-source operating system. Linus Torvalds is a original creator of the Linux operating system.Any changes to it are open for all to adopt, and as a result it has developed into a very powerful/security/virus-free OS that is rapidly gaining in popularity worldwide, particularly among those seeking an alternative to Windows.Linux is free to use and install, and is more reliable than almost all other systems, running for many months and even years without a reboot being necessary,so now mostly used operating system for PC's and also more suitable for IT business.</p> <p>Linux operating system is made up of three parts, such as Kernel,Shell,Programs.</p>","tags":["Learn","Linux","Opensource"]},{"location":"linux/#kernel","title":"Kernel","text":"<p>Kernel is the main part of Linux operating system and it is acting as hub.It allocates time,resources such as memory to programs and handles the file-store and communications in response to system calls.Kernel receive the request the command from the shell and do the operation accordingly by allocating necessary resources.</p>","tags":["Learn","Linux","Opensource"]},{"location":"linux/#shell","title":"Shell","text":"<p>Shell is an interface between the kernel and the program application. It would gather the operation/inputs from the user/application programs and pass the operations to the kernel to perform. And once operation completed by the kernel and pass the output to the user accordingly.</p>","tags":["Learn","Linux","Opensource"]},{"location":"linux/#programs","title":"Programs","text":"<p>Programs is the set of code to do the operation required for the user and it can customized as per our need and developed and produced into user-level to utilized. Most of the linux programs also come along with GNU public license ,so user can customize and compile the programs as own wish.</p>","tags":["Learn","Linux","Opensource"]},{"location":"linux/#advantages-and-benefits-of-linux","title":"Advantages and Benefits of Linux","text":"<ul> <li>No proprietary/Owner.it can be debugged without resource to a license owner or software proprietor as per our own wish.</li> <li>Flexibility - We can customize the Linux OS as per our own/company wish to adapt with our business needs.</li> <li>Free of Cost - Core OS and related software applications comes under GNU general public license.we can download the OS and related application at free of cost.</li> <li>It is free to use and distribute.</li> <li>Support is free through online help sites, blogs and forums.</li> <li>It is very reliable - Linux distributions getting lower crashes compare to other operating systems.</li> <li>Large number of software developed for Linux under open-source license.</li> <li>It is very resistant to malware such as spyware, adware and viruses.</li> <li>It runs in a wide variety of machines than cannot be updated to use newer Windows versions.Adapted with almost all the type of hardware's.</li> <li>Since the source code is visible, 'backdoors' are easily spotted, so Linux offers greater security for sensitive applications.</li> <li>Linux offers a high degree of flexibility of configuration, and significant customization is possible without modifying the source code.</li> </ul>","tags":["Learn","Linux","Opensource"]},{"location":"linux/administration/how_to_disable_ipv6_on_centos/","title":"How to Disable IPv6 on CentOS","text":"","tags":["CentOS","ipv6","disable ipv6"]},{"location":"linux/administration/how_to_disable_ipv6_on_centos/#introduction","title":"Introduction","text":"<p>IPv6 is the next-generation internet protocol that provides a larger address space compared to IPv4. However, there may be situations where you need to disable IPv6 on your CentOS server. Disabling IPv6 can help resolve compatibility issues or improve network performance. This guide provides two methods to disable IPv6 on CentOS: using sysctl and using the kernel boot option. By following these steps, you can effectively disable IPv6 on your CentOS system.</p>","tags":["CentOS","ipv6","disable ipv6"]},{"location":"linux/administration/how_to_disable_ipv6_on_centos/#method-1-disable-ipv6-using-sysctl","title":"Method 1: Disable IPv6 Using sysctl","text":"<ol> <li> <p>Open the sysctl configuration file /etc/sysctl.d/90-ipv6.conf using the following command:</p> Bash Session<pre><code>vi /etc/sysctl.d/90-ipv6.conf\n</code></pre> </li> <li> <p>Add the following lines to the file and save it:</p> Text Only<pre><code>net.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n</code></pre> </li> <li> <p>Apply the changes by running the following command:</p> Bash Session<pre><code>sysctl --load /etc/sysctl.d/90-ipv6.conf\n</code></pre> </li> <li> <p>To verify that IPv6 is disabled, use the following command:</p> Bash Session<pre><code>ip a | grep inet6\n</code></pre> <p>If the command does not return any output, it means that IPv6 has been disabled on all your network interfaces.</p> </li> </ol> <p>Note</p> <p>If you are using CentOS 8 with Network Manager, some network interfaces may still use IPv6 after a system reboot. To completely stop using IPv6, proceed to Method 2.</p>","tags":["CentOS","ipv6","disable ipv6"]},{"location":"linux/administration/how_to_disable_ipv6_on_centos/#method-2-disable-ipv6-using-the-kernel-boot-option","title":"Method 2: Disable IPv6 Using the Kernel Boot Option","text":"<ol> <li> <p>Open the default GRUB configuration file <code>/etc/default/grub</code> using the following command:</p> Bash Session<pre><code>vi /etc/default/grub\n</code></pre> </li> <li> <p>Locate the line that begins with <code>GRUB_CMDLINE_LINUX</code> and append <code>ipv6.disable=1</code> to the existing parameters. The line should look like this:</p> VimL<pre><code>GRUB_CMDLINE_LINUX=\"$GRUB_CMDLINE_LINUX ipv6.disable=1\"\n</code></pre> </li> <li> <p>Save and exit the configuration file.</p> </li> <li> <p>Update the GRUB configuration files by running the following commands:</p> Bash Session<pre><code>grub2-mkconfig -o /boot/grub2/grub.cfg\ngrub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg\n</code></pre> </li> <li> <p>Reboot your CentOS machine for the changes to take effect:</p> Bash Session<pre><code>reboot\n</code></pre> </li> <li> <p>After the reboot, use the following command to verify whether IPv6 is disabled:</p> Bash Session<pre><code>ip a | grep inet6\n</code></pre> <p>If no IPv6 addresses are displayed, it indicates that IPv6 has been successfully disabled.</p> </li> </ol>","tags":["CentOS","ipv6","disable ipv6"]},{"location":"linux/administration/how_to_disable_ipv6_on_centos/#conclusion","title":"Conclusion","text":"<p>Disabling IPv6 on CentOS can be necessary in certain scenarios. By following the methods outlined in this guide, you can effectively disable IPv6 on your CentOS system. Choose the method that best suits your requirements and verify the success of the disabling process. Disabling IPv6 can help resolve compatibility issues and improve network performance if IPv6 is not required in your environment.</p>","tags":["CentOS","ipv6","disable ipv6"]},{"location":"linux/administration/how_to_mount_qcow2_image_in_linux/","title":"How to Mount a qcow2 Disk Image","text":"","tags":["Learn","Linux","qcow2","mount qcow2"]},{"location":"linux/administration/how_to_mount_qcow2_image_in_linux/#introduction","title":"Introduction","text":"<p>Mounting a qcow2 disk image on your host server allows you to perform various operations such as password resets, file editing, or data recovery without running the virtual machine. This guide provides step-by-step instructions on how to mount a qcow2 disk image, enabling you to access and manipulate its contents directly on the host server.</p>","tags":["Learn","Linux","qcow2","mount qcow2"]},{"location":"linux/administration/how_to_mount_qcow2_image_in_linux/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the steps, ensure that you have the following:</p> <ul> <li>Administrative access to the host server.</li> <li>The qemu-nbd tool installed on the host server.</li> <li>The qcow2 disk image file you want to mount.</li> </ul>","tags":["Learn","Linux","qcow2","mount qcow2"]},{"location":"linux/administration/how_to_mount_qcow2_image_in_linux/#mounting-a-qcow2-disk-image","title":"Mounting a qcow2 Disk Image","text":"<p>Follow these steps to mount a qcow2 disk image on your host server:</p> <ol> <li> <p>Enable NBD (Network Block Device) on the host server by running the following command:</p> Bash Session<pre><code>modprobe nbd max_part=8\n</code></pre> </li> <li> <p>Connect the qcow2 disk image file as a network block device using the qemu-nbd command. Replace /path/to/image.qcow2 with the actual path to your qcow2 disk image file:</p> Bash Session<pre><code>qemu-nbd --connect=/dev/nbd0 /path/to/image.qcow2\n</code></pre> </li> <li> <p>Identify the partitions within the virtual machine disk image by running the fdisk command:</p> Bash Session<pre><code>fdisk /dev/nbd0 -l\n</code></pre> <p>This will display a list of partitions present in the qcow2 disk image.</p> </li> <li> <p>Choose the partition you want to mount from the virtual machine disk image. For example, to mount the first partition (usually /dev/nbd0p1), create a mount point directory (e.g., /mnt/somepoint) and run the following command:</p> Bash Session<pre><code>mount /dev/nbd0p1 /mnt/somepoint/\n</code></pre> <p>If you encounter an error like \"unknown filesystem type 'LVM2_member',\" follow the instructions below:</p> <ul> <li> <p>Load the dm-mod kernel module by running the command</p> Bash Session<pre><code>modprobe dm-mod\n</code></pre> </li> <li> <p>Use the vgdisplay command to obtain the UUID of the volume group (VG), and then execute the vgrename command to rename it</p> Bash Session<pre><code>vgdisplay\nvgrename vg_UUID new_vgname\n</code></pre> </li> <li> <p>Mount the LVM physical volume (PV) to the mount point using the new VG name</p> Bash Session<pre><code>mount /dev/new_vgname/root /mnt/\n</code></pre> </li> </ul> </li> <li> <p>Once you have finished working with the mounted partition, unmount it by running the following command:</p> Bash Session<pre><code>umount /mnt/somepoint/\n</code></pre> </li> <li> <p>Disconnect the qcow2 disk image from the network block device by running the following command:</p> Bash Session<pre><code>qemu-nbd --disconnect /dev/nbd0\n</code></pre> </li> <li> <p>If you no longer need the NBD module, unload it from the kernel by running the following command:</p> Bash Session<pre><code>rmmod nbd\n</code></pre> </li> </ol>","tags":["Learn","Linux","qcow2","mount qcow2"]},{"location":"linux/administration/how_to_mount_qcow2_image_in_linux/#conclusion","title":"Conclusion","text":"<p>Mounting a qcow2 disk image allows you to access and modify its contents directly on the host server, providing flexibility for various maintenance and recovery tasks. By following the steps outlined in this guide, you can successfully mount a qcow2 disk image and perform necessary operations without running the associated virtual machine.</p>","tags":["Learn","Linux","qcow2","mount qcow2"]},{"location":"linux/administration/linux_users_and_groups_management/","title":"Users and Groups in Linux/Unix: Managing Access and Privileges for Multiple Users","text":"","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#users-and-groups","title":"Users and Groups","text":"<p>Linux/Unix operating systems have the ability to multitask in a manner similar to other operating systems. However, Linux\u2019s major difference from other operating systems is its ability to have multiple users. Linux was designed to allow more than one user to have access to the system at the same time.</p> <p>The user of the system is either a human being or an account used by specific applications identified by a unique numerical identification number called user ID (UID). Users within a group can have read permissions, write permissions, execute permissions or any combination of read/write/execute permissions for files owned by that group.</p> <p>A group is an organization unit tying users together for a common purpose, which can be reading permissions, writing permission, or executing permission for files owned by that group. Similar to UID, each group is associated with a group ID (GID).</p>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#etcpasswd","title":"/etc/passwd","text":"<p>It stores user account information, which is required during login.It contains a list of the system's accounts, giving for each account some useful information like user ID, group ID, home directory, shell, etc. It should readable by many utilities, but write access only for the root user's.</p> <p>Each entry on this file should be mentioned below ::</p> <p>Note</p> <p>ganapathi:x:501:501:Ganapathi Chidambaram:/home/ganapathi:/bin/bash</p> <ol> <li> <p>Username: It is username which is used to logs into the system. It should be between 1 and 32 characters in length.</p> </li> <li> <p>Password: An x character indicates that password is encrypted and saved password would be available in /etc/shadow file.</p> </li> <li> <p>User ID (UID): The numerical equivalent of the username which is referenced by the system and applications when determining access privileges.Each user must be assigned a user ID (UID). UID 0 (zero) is reserved for root and UIDs 1-99 are reserved for other predefined accounts. Further UID 100-999 are reserved by system for administrative and system accounts/groups.And UID reservation range can be able to modify through /etc/login.defs.</p> </li> <li> <p>Group ID (GID): The numerical equivalent of the primary group name which is referenced by the system and applications when determining access privileges.The primary group ID for the user.</p> </li> <li> <p>User ID Info: The comment field. It allow you to add extra information about the users such as user's full name, phone number etc. This field use by finger command.</p> </li> <li> <p>Home directory: The absolute path to the directory the user will be in when they log in. If this directory does not exists then users directory becomes /</p> </li> <li> <p>Command/shell: The program automatically launched whenever a user logs in. This is usually a command interpreter (often called a shell). Under Red Hat Enterprise Linux, the default value is /bin/bash. If this field is left blank, /bin/sh is used. If it is set to a non-existent file, then the user will be unable to log into the system.</p> </li> </ol>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#etcshadow","title":"/etc/shadow","text":"<p>Individual users on the system are provided with shadow passwords. Encrypted password hashes are stored in here, which is readable only by the root user, and store information about password aging as well.</p> <p>Each entry on this file should be mentioned below</p> <p>Note</p> Text Only<pre><code>ganapathi:$1$.QKDPc5E$SWlkjRWexrXYgc98F.:12825:0:90:5:30:13096:\n</code></pre> <p>This line shows the following information for user ganapathi:</p> <ul> <li>The password was last changed February 11, 2005</li> <li>There is no minimum amount of time required before the password can be changed</li> <li>The password must be changed every 90 days</li> <li>The user will get a warning five days before the password must be changed</li> <li>The account will be disabled 30 days after the password expires if no login attempt is made</li> <li>The account will expire on November 9,2005.</li> </ul> <ol> <li> <p>Username: Username to login into the system.</p> </li> <li> <p>Password: The 13 to 24 character password. The password is encrypted using either the crypt library function  or the md5 hash algorithm. In this field, values other than a validly-formatted encrypted or hashed password are used to control user logins and to show the password status. For example, if the value is <code>!</code> or <code>*</code>, the account is locked and the user is not allowed to log in. If the value is !! a password has never been set before (and the user, not having set a password, will not be able to log in).</p> </li> <li> <p>Last password change (lastchanged) : The number of days since January 1, 1970 (also called the epoch) that the password was last changed. This information is used in conjunction with the password aging fields that follow.</p> </li> <li> <p>Minimum : The minimum number of days that must pass before the password can be changed.</p> </li> <li> <p>Maximum : The number of days that must pass before the password must be changed.</p> </li> <li> <p>Warn : The number of days before password expiration during which the user is warned of the impending expiration.</p> </li> <li> <p>Inactive : The number of days after a password expires before the account will be disabled.</p> </li> <li> <p>Expire :  The date (stored as the number of days since the epoch) since the user account has been disabled.</p> </li> </ol>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#etcgroup","title":"/etc/group","text":"<p>It defines the groups to which users belong under Linux and UNIX operating system. Under Unix / Linux multiple users can be categorized into groups.The use of groups allows additional abilities to be delegated in an organized fashion, such as access to disks, printers, and other peripherals. This method, amongst others, also enables the Superuser to delegate some administrative tasks to normal users.</p> <p>Each entry on this file should be mentioned below</p> <p>Note</p> <p>ganapathi:x:24:ganapathi,raja</p> <ol> <li> <p>group_name: It is the name of group. If you run ls -l command, you will see this name printed in the group field.</p> </li> <li> <p>Password: Generally password is not used, hence it is empty/blank. It can store encrypted password. This is useful to implement privileged groups.</p> </li> <li> <p>Group ID (GID): The numerical equivalent of the group name. It is used by the operating system and applications when determining access privileges.</p> </li> <li> <p>Group List : It is a list of user names of users who are members of the group. The user names, must be separated by commas.</p> </li> </ol>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#etcgshadow","title":"/etc/gshadow","text":"<p>Each Group on the system are provided with shadow passwords. Encrypted password hashes are stored in here, which is readable only by the root user group, and store information about password aging as well.</p> <p>Each entry on this file should be mentioned below ::</p> <p>Note</p> <p>ganapathi:x:ganapathi:ganapathi,raja</p> <ol> <li> <p>Group name \u2014 The name of the group. Used by various utility programs as a human-readable identifier for the group.</p> </li> <li> <p>Encrypted password \u2014 The encrypted password for the group. If set, non-members of the group can join the group by typing the password for that group using the newgrp command. If the value of this field is !, then no user is allowed to access the group using the newgrp command. A value of !! is treated the same as a value of ! \u2014 however, it also indicates that a password has never been set before. If the value is null, only group members can log into the group.</p> </li> <li> <p>Group administrators \u2014 Group members listed here (in a comma delimited list) can add or remove group members using the gpasswd command.</p> </li> <li> <p>Group members \u2014 Group members listed here (in a comma delimited list) are regular, non-administrative members of the group</p> </li> </ol>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#etclogindefs","title":"/etc/login.defs","text":"<p>Under Linux password related utilities and config file(s) comes from shadow password suite.It defines the site-specific configuration for this suite.The lines consist of a configuration name and value, separated by whitespace.Absence of this file will not prevent system operation, but will probably result in undesirable operation.</p> <p>Blank lines and comment lines are ignored. Comments are introduced with a \u201c#\u201d pound sign and the pound sign must be the first non-white character of the line.</p> <p>Parameter values may be of four types: strings, booleans, numbers, and long numbers. A string is comprised of any printable characters. A boolean should be either the value \u201cyes\u201d or \u201cno\u201d. An undefined boolean parameter or one with a value other than these will be given a \u201cno\u201d value. Numbers (both regular and long) may be either decimal values, octal values (precede the value with \u201c0\u201d) or hexadecimal values (precede the value with \u201c0x\u201d). The maximum value of the regular and long numeric parameters is machine-dependent.</p> <p>The following configuration items are provided:</p> <ul> <li> <p>CHFN_AUTH (boolean) :    If yes, the chfn and chsh programs will require authentication before making any changes, unless run by the superuser.</p> </li> <li> <p>CHFN_RESTRICT (string) :   This parameter specifies which values in the gecos field of the /etc/passwd file may be changed by regular users using the chfn program. It can be any combination of letters f ,r, w, h, for Full name, Room number, Work phone, and Home phone, respectively. For backward compatibility, \u201cyes\u201d is equivalent to \u201crwh\u201d and \u201cno\u201d is equivalent to \u201cfrwh\u201d. If not specified, only the superuser can make any changes. The most restrictive setting is better achieved by not installing chfn SUID.</p> </li> <li> <p>DEFAULT_HOME : Should login be allowed if we can't cd to the home directory?. Default in no.So change it as Yes for cd to the home directory.</p> </li> <li> <p>ENCRYPT_METHOD : Encryption method for the password entered by the user.More complicated algorithm is used to difficult to brute forcing the password.</p> </li> <li> <p>ENV_PATH &amp; ENV_SUPATH - Default Environment path for normal user and super user login.This is must be defined to successful login of user into the system.</p> </li> <li> <p>FAILLOG_ENABLE : Enable logging and display of /var/log/faillog login failure info.</p> </li> <li> <p>LOGIN_RETRIES : Max number of login retries if password is bad. This will most likely be overriden by PAM, since   the default pam_unix module has it's own built in of 3 retries. However, this is a safe fallback in case you are   using an authentication module that does not enforce PAM_MAXTRIES.</p> </li> <li> <p>LOGIN_TIMEOUT : Max time in seconds for login.</p> </li> <li> <p>GID_MAX (number), GID_MIN (number) : Range of group IDs to choose from for the useradd and groupadd programs.</p> </li> <li> <p>MAIL_DIR (string) : The mail spool directory. This is needed to manipulate the mailbox when its corresponding user account is modified or deleted. If not specified, a compile-time default is used.</p> </li> <li> <p>PASS_MAX_DAYS (number) : The maximum number of days a password may be used. If the password is older than this, a password change will be forced. If not specified, -1 will be assumed (which disables the restriction).</p> </li> <li> <p>PASS_MIN_DAYS (number) :  The minimum number of days allowed between password changes. Any password changes attempted sooner than this will be rejected. If not specified, -1 will be assumed (which disables the restriction).</p> </li> <li> <p>PASS_WARN_AGE (number) :  The number of days warning given before a password expires. A zero means warning is given only upon the day of expiration, a negative value means no warning is given. If not specified, no warning will be provided.</p> </li> <li> <p>UID_MAX (number), UID_MIN (number) : Range of user IDs to choose from for the useradd program.</p> </li> <li> <p>UMASK (number) : The permission mask is initialized to this value. If not specified, the permission mask will be initialized to 022.</p> </li> <li> <p>USERDEL_CMD (string) :    If defined, this command is run when removing a user. It should remove any at/cron/print jobs etc. owned by the user to be removed (passed as the first argument).</p> </li> </ul> <p>The following cross reference shows which programs in the shadow password suite use which parameters.</p> <ul> <li>chfn  CHFN_AUTH CHFN_RESTRICT</li> <li>chsh   CHFN_AUTH</li> <li>groupadd   GID_MAX GID_MIN</li> <li>newusers  PASS_MAX_DAYS PASS_MIN_DAYS PASS_WARN_AGE UMASK</li> <li>pwconv  PASS_MAX_DAYS PASS_MIN_DAYS PASS_WARN_AGE</li> <li>useradd   GID_MAX GID_MIN PASS_MAX_DAYS PASS_MIN_DAYS PASS_WARN_AGE UID_MAX UID_MIN UMASK</li> <li>userdel  MAIL_DIR USERDEL_CMD</li> <li>usermod MAIL_DIR</li> </ul>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#adding-a-new-user","title":"Adding a New User","text":"<p>To create a new standard user, we should use the <code>useradd</code> command and for the syntax is as follows:</p> Bash Session<pre><code>useradd &lt;name&gt;\nadduser &lt;name&gt;\n</code></pre> <p>These mentioned Process would happen for Every newly created user.</p> <ul> <li>Would create the Home Directory for the user.</li> <li>Copy the Below mentioned Files on their Home Directory.       Bash Session<pre><code>.bash_logout\n.bash_profile\n.bashrc\n</code></pre></li> <li>Would create the Mail Spool directory for the user.</li> <li>A group would create automatically in the same name of user.</li> </ul> <p>adduser command is an interactive mode of user creation.Along with user creation would ask for password to set,and gecos information for the user(Full Name,Room Number,Work Phone,Home Phone) to store into /etc/passwd file.</p>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#useradd-options","title":"Useradd Options","text":"<ul> <li> <p>-d --home  <p>home_dir will be used as the value for the user\u2019s login directory</p> Bash Session<pre><code>  useradd &lt;name&gt; -d /home/&lt;user's home&gt;\n</code></pre> <li> <p>-e --expiredate  <p>the date when the account will expire</p> Bash Session<pre><code>useradd &lt;name&gt;** -e &lt;YYYY-MM-DD&gt;\n</code></pre> <li> <p>-f --inactive  <p>the number of days before the account expires</p> Bash Session<pre><code>useradd &lt;name&gt; -f &lt;0 or -1&gt;\n</code></pre> <li> <p>-k, --skel  <p>The skeleton directory, which contains files and directories to be copied in the user's home directory, when the home directory is created by useradd.</p> <p>This option is only valid if the -m (or --create-home) option is specified.</p> <p>If this option is not set, the skeleton directory is defined by the SKEL variable in /etc/default/useradd or, by default, /etc/skel.</p> <p>If possible, the ACLs and extended attributes are copied.</p> <li> <p>-K, --key KEY=VALUE</p> <p>Overrides /etc/login.defs defaults (UID_MIN, UID_MAX, UMASK, PASS_MAX_DAYS and others).</p> <p>Example:</p> <p>-K PASS_MAX_DAYS=-1 can be used when creating system account to turn off password ageing, even though system           account has no password at all. Multiple -K options can be specified, e.g.: -K UID_MIN=100-K UID_MAX=499</p> </li> <li> <p>-k, --skel SKEL_DIR</p> <p>The skeleton directory, which contains files and directories to be copied in the user's home directory, when the home directory is created by useradd.</p> <p>This option is only valid if the -m (or --create-home) option is specified.</p> <p>If this option is not set, the skeleton directory is defined by the SKEL variable in /etc/default/useradd or, by default, /etc/skel.</p> <p>If possible, the ACLs and extended attributes are copied.</p> </li> <li> <p>-m, --create-home</p> <p>Create the user's home directory if it does not exist. The files and directories contained in the skeleton directory (which can be defined with the -k option) will be copied to the home directory.</p> <p>By default, if this option is not specified and CREATE_HOME is not enabled, no home directories are created.</p> </li> <li> <p>-M</p> </li> <p>Do no create the user's home directory, even if the system wide setting from /etc/login.defs (CREATE_HOME) is set to yes.</p> <ul> <li> <p>-N, --no-user-group</p> <p>Do not create a group with the same name as the user, but add the user to the group specified by the -g option or by the GROUP variable in /etc/default/useradd.</p> <p>The default behavior (if the -g, -N, and -U options are not specified) is defined by the USERGROUPS_ENAB variable in /etc/login.defs.</p> </li> <li> <p>-o, --non-unique</p> <p>Allow the creation of a user account with a duplicate (non-unique) UID.</p> <p>This option is only valid in combination with the -u option.</p> </li> <li> <p>-p, --password PASSWORD</p> <p>The encrypted password, as returned by crypt(3). The default is to disable the password.</p> <p>Note: This option is not recommended because the password (or encrypted password) will be visible by users listing the processes.</p> <p>You should make sure the password respects the system's password policy.</p> </li> <li> <p>-r, --system</p> <p>Create a system account.</p> <p>System users will be created with no aging information in /etc/shadow, and their numeric identifiers are chosen in the SYS_UID_MIN-SYS_UID_MAX range, defined in /etc/login.defs, instead of UID_MIN-UID_MAX (and their GID counterparts for the creation of groups).</p> <p>Note that useradd will not create a home directory for such an user, regardless of the default setting in /etc/login.defs (CREATE_HOME). You have to specify the -m options if you want a home directory for a system account to be created.</p> </li> <li> <p>-R, --root CHROOT_DIR</p> <p>Apply changes in the CHROOT_DIR directory and use the configuration files from the CHROOT_DIR directory.</p> </li> <li> <p>-s, --shell SHELL</p> <p>The name of the user's login shell. The default is to leave this field blank, which causes the system to select the default login shell specified by the SHELL variable in /etc/default/useradd, or an empty string by default.</p> Bash Session<pre><code>useradd &lt;name&gt; -s /bin/&lt;shell&gt;\n</code></pre> </li> <li> <p>-u, --uid UID</p> <p>The numerical value of the user's ID. This value must be unique, unless the -o option is used. The value must be non-negative. The default is to use the smallest ID value greater than or equal to UID_MIN and greater than every other user.</p> <p>See also the -r option and the UID_MAX description.</p> </li> <li> <p>-U, --user-group</p> <p>Create a group with the same name as the user, and add the user to this group.</p> <p>The default behavior (if the -g, -N, and -U options are not specified) is defined by the USERGROUPS_ENAB variable in   /etc/login.defs.</p> </li> </ul>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#adduser-options","title":"adduser Options","text":"<ul> <li> <p>--conf FILE</p> <p>Use FILE instead of /etc/adduser.conf.</p> </li> <li> <p>--disabled-login</p> <p>Do not run passwd to set the password.  The user won't be able to use her account until the password is set.</p> </li> <li> <p>--disabled-password</p> <p>Like --disabled-login, but logins are still possible (for example using SSH RSA keys) but not using password authentication.</p> </li> <li> <p>--force-badname</p> <p>By  default,  user  and  group names are checked against the configurable regular expression NAME_REGEX (or NAME_REGEX_SYSTEM if --system is specified) specified in the configuration file. This option forces adduser and addgroup to apply only a  weak  check for validity of the name.</p> </li> <li> <p>--gecos GECOS</p> <p>Set the gecos field for the new entry generated.  adduser will not ask for finger information if this option is given.</p> </li> <li> <p>--gid ID</p> <p>When  creating  a  group, this option forces the new groupid to be the given number.  When creating a user, this option will put   the user in that group.</p> </li> <li> <p>--group</p> <p>When combined with --system, a group with the same name and ID as the system user is created.  If not combined with --system,  a group with the given name is created.  This is the default action if the program is invoked as addgroup.</p> </li> <li> <p>--home DIR</p> <p>Use  DIR  as  the user's home directory, rather than the default specified by the configuration file.  If the directory does not exist, it is created and skeleton files are copied.</p> </li> </ul>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#adding-a-group","title":"Adding a Group","text":"<p>To add a group to the system, use the command groupadd/addgroup and syntax as follows :</p> Bash Session<pre><code>    groupadd &lt;group-name&gt;\n    addgroup &lt;group-name&gt;\n</code></pre>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#groupadd-options","title":"groupadd Options","text":"<ul> <li> <p>-g <p>Group ID for the group, which must be unique and greater than 499</p> <li> <p>-r</p> <p>Create a system group with a GID less than 500</p> </li> <li> <p>-f</p> <p>When used with -g and  already exists, groupadd will choose another unique  for the group.","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#addgroup-options","title":"addgroup Options","text":"<ul> <li> <p>-g <p>Group ID for the group, which must be unique and greater than 499</p>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#user-modification","title":"User Modification","text":"<p>By usermod command we can modify the existing available users on the system with help of below configuration.</p> <ul> <li> <p>-a, --append</p> <p>Add the user to the supplementary group(s). Use only with the -G option.</p> </li> <li> <p>-c, --comment COMMENT</p> <p>The new value of the user's password file comment field. It is normally modified using the chfn(1) utility.</p> </li> <li> <p>-d, --home HOME_DIR</p> <p>The user's new login directory.</p> <p>If the -m option is given, the contents of the current home directory will be moved to the new home directory, which is created if it does not already exist.</p> </li> <li> <p>-e, --expiredate EXPIRE_DATE</p> <p>The date on which the user account will be disabled. The date is specified in the format YYYY-MM-DD.</p> <p>An empty EXPIRE_DATE argument will disable the expiration of the account.</p> <p>This option requires a /etc/shadow file. A /etc/shadow entry will be created if there were none.</p> </li> <li> <p>-f, --inactive INACTIVE</p> <p>The number of days after a password expires until the account is permanently disabled.</p> <p>A value of 0 disables the account as soon as the password has expired, and a value of -1 disables the feature.</p> <p>This option requires a /etc/shadow file. A /etc/shadow entry will be created if there were none.</p> </li> <li> <p>-g, --gid GROUP</p> <p>The group name or number of the user's new initial login group. The group must exist.</p> <p>Any file from the user's home directory owned by the previous primary group of the user will be owned by this new group.</p> <p>The group ownership of files outside of the user's home directory must be fixed manually.</p> </li> <li> <p>-G, --groups GROUP1[,GROUP2,...[,GROUPN]]]</p> <p>A list of supplementary groups which the user is also a member of. Each group is separated from the next by a comma, with no intervening whitespace. The groups are subject to the same restrictions as the group given with the -g option.</p> <p>If the user is currently a member of a group which is not listed, the user will be removed from the group. This behaviour can be changed via the -a option, which appends the user to the current supplementary group list.</p> </li> <li> <p>-l, --login NEW_LOGIN</p> <p>The name of the user will be changed from LOGIN to NEW_LOGIN. Nothing else is changed. In particular, the user's home directory or mail spool should probably be renamed manually to reflect the new login name.</p> </li> <li> <p>-L, --lock</p> <p>Lock a user's password. This puts a '!' in front of the encrypted password, effectively disabling the password. You can't use this option with -p or -U.</p> <p>Note</p> <p>if you wish to lock the account (not only access with a password), you should also set the EXPIRE_DATE to 1.</p> </li> <li> <p>-m, --move-home</p> <p>Move the content of the user's home directory to the new location.</p> <p>This option is only valid in combination with the -d (or --home) option.</p> <p>usermod will try to adapt the ownership of the files and to copy the modes, ACL and extended attributes, but manual changes might be needed afterwards.</p> </li> <li> <p>-o, --non-unique</p> <p>When used with the -u option, this option allows to change the user ID to a non-unique value.</p> </li> <li> <p>-p, --password PASSWORD</p> <p>The encrypted password, as returned by crypt(3).</p> <p>Note</p> <p>This option is not recommended because the password (or encrypted password) will be visible by users listing the processes.</p> <p>The password will be written in the local /etc/passwd or /etc/shadow file. This might differ from the password database configured in your PAM configuration.</p> </li> </ul>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/linux_users_and_groups_management/#password-aging","title":"Password Aging","text":"<p>For security reasons, it is advisable to require users to change their passwords periodically.To configure password expiration for a user from a shell prompt, use the chage command, followed by an option which is mentioned below ,followed by the username of the user.</p> <ul> <li> <p>-m <p>Specifies the minimum number of days between which the user must change passwords. If the value is 0, the password does not expire.</p> <li> <p>-M <p>Specifies the maximum number of days for which the password is valid. When the number of days specified by this option plus the number of days specified with the -d option is less than the current day, the user must change passwords before using the account.</p> <li> <p>-d <p>Specifies the number of days since January 1, 1970 the password was changed</p> <li> <p>-I <p>Specifies the number of inactive days after the password expiration before locking the account. If the value is 0, the account is not locked after the password expires.</p> <li> <p>-E <p>Specifies the date on which the account is locked, in the format YYYY-MM-DD. Instead of the date, the number of days since January 1, 1970 can also be used.</p> <li> <p>-W <p>Specifies the number of days before the password expiration date to warn the user.</p>","tags":["Users","Linux Users","User Management","Linux Access"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/","title":"Recursive Directory Listing in C using Linux System Calls","text":"","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#introduction","title":"Introduction","text":"<p>In this article, we will explore a C code that utilizes Linux system calls to recursively list directories and their contents. The code provides the flexibility to filter the listing based on file types and specify the depth of recursion. Let's dive into the details of the code.</p> C<pre><code>#define _GNU_SOURCE\n#include &lt;dirent.h&gt;     /* Defines DT_* constants */\n#include &lt;fcntl.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;sys/syscall.h&gt;\n\n#define handle_error(msg) \\\n        do { perror(msg); exit(EXIT_FAILURE); } while (0)\n\nstruct linux_dirent {\n    long           d_ino;\n    off_t          d_off;\n    unsigned short d_reclen;\n    char           d_name[];\n};\n\n#define BUF_SIZE 1024*1024*5\n\nvoid listdir(char *dname, char *otype, int inward) {\n    int fd, nread;\n    char *buf = malloc(BUF_SIZE);\n    struct linux_dirent *d;\n    int bpos;\n    char d_type;\n\n    fd = open(dname != NULL ? dname : \".\", O_RDONLY | O_DIRECTORY);\n\n    if (fd == -1)\n        handle_error(\"open\");\n\n    for (;;) {\n        nread = syscall(SYS_getdents, fd, buf, BUF_SIZE);\n        if (nread == -1)\n            handle_error(\"getdents\");\n        if (nread == 0)\n            break;\n\n        for (bpos = 0; bpos &lt; nread;) {\n            d = (struct linux_dirent *)(buf + bpos);\n            d_type = *(buf + bpos + d-&gt;d_reclen - 1);\n            bpos += d-&gt;d_reclen;\n\n            if (d-&gt;d_ino &amp;&amp; d-&gt;d_ino &gt; 0 &amp;&amp; strcmp(\".\", d-&gt;d_name) != 0 &amp;&amp; strcmp(\"..\", d-&gt;d_name) != 0) {\n                if ((d_type == DT_DIR || d_type == DT_LNK) &amp;&amp; (strcmp(\"dirs\", otype) == 0 || strcmp(\"dirfiles\", otype) == 0))\n                    printf(\"%s/%s\\n\", dname, (char *)d-&gt;d_name);\n                else if (d_type == DT_REG &amp;&amp; (strcmp(\"files\", otype) == 0 || strcmp(\"dirfiles\", otype) == 0))\n                    printf(\"%s/%s\\n\", dname, (char *)d-&gt;d_name);\n\n                if ((d_type == DT_DIR || d_type == DT_LNK) &amp;&amp; inward == 1) {\n                    int dirname_len = strlen(dname);\n                    char *subdir = calloc(1, PATH_MAX + 1);\n                    strcat(subdir, dname);\n                    strcat(subdir + dirname_len, \"/\");\n                    strcat(subdir + dirname_len + 1, d-&gt;d_name);\n                    listdir(subdir, otype, inward);\n                    free(subdir);\n                }\n            }\n        }\n    }\n\n    close(fd);\n    free(buf);\n}\n\nint main(int argc, char *argv[]) {\n    int opt = 0;\n    char *dname = NULL;\n    char *otype = NULL;\n    int inward = 0;\n\n    while ((opt = getopt(argc, argv, \"d:t:i:\")) != -1) {\n        switch (opt) {\n            case 'd':\n                dname = optarg;\n                break;\n            case 't':\n                otype = optarg;\n                break;\n            case 'i':\n                inward = atoi(optarg) == 1 ? atoi(optarg) : 0;\n                break;\n            case '?':\n                if (optopt == 'd')\n                    dname = \".\";\n                else if (optopt == 't')\n                    otype = \"files\";\n                else {\n                    printf(\"\\nInvalid option received\\n\");\n                    return 1;\n                }\n                break;\n        }\n    }\n\n    listdir(dname, otype, inward);\n    exit(EXIT_SUCCESS);\n    return 0;\n}\n</code></pre>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#benefits-of-this-code","title":"Benefits of this Code","text":"<p>The provided C code provides comparable functionality to the find and ls commands in Linux. However, this C program is specifically designed to efficiently handle large directories with a substantial number of files, ensuring it doesn't exhaust CPU and memory resources. Unlike find and ls, which may struggle on lower-end systems or when dealing with millions of files, this C program is capable of successfully executing and delivering results in such scenarios.</p> <ol> <li> <p>Memory Utilization:</p> <ul> <li> <p>Efficient Memory Management: The C code utilizes dynamic memory allocation (malloc and free) to manage the buffer size (<code>buf</code>). This allows for more efficient memory utilization, especially when dealing with large directory structures, as the code can allocate memory as needed.</p> </li> <li> <p>Fixed Buffer Size: The buffer size (<code>BUF_SIZE</code>) in the C code is set to 5 megabytes (<code>1024*1024*5</code>). This fixed buffer size ensures that a reasonable amount of memory is allocated upfront, preventing excessive memory consumption.</p> </li> </ul> </li> <li> <p>CPU Utilization:</p> <ul> <li> <p>System Calls: The C code directly uses system calls like <code>open</code>, <code>getdents</code>, and <code>close</code> to interact with the file system. By utilizing low-level system calls, the code avoids the overhead associated with executing external commands (<code>find</code> or <code>ls</code>), resulting in potentially lower CPU utilization.</p> </li> <li> <p>Tailored Logic: The C code contains custom logic for filtering and processing directory entries. It only performs the necessary checks and actions as specified by the options (-t and -i). This targeted approach can lead to improved CPU utilization compared to the more generic behavior of <code>find</code> and <code>ls</code> commands, which perform a broader set of operations by default.</p> </li> </ul> </li> </ol>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#understanding-the-code","title":"Understanding the Code","text":"<p>The code provided is a C program that allows recursive directory listing in a Linux environment. Let's go through the important parts of the code to understand its functionality.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#header-files-and-macros","title":"Header Files and Macros","text":"<p>The code starts by including necessary header files such as <code>dirent.h</code>, <code>fcntl.h</code>, <code>stdio.h</code>, <code>unistd.h</code>, <code>stdlib.h</code>, <code>string.h</code>, <code>sys/stat.h</code>, and <code>sys/syscall.h</code>. Additionally, the <code>_GNU_SOURCE</code> macro is defined.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#error-handling-macro","title":"Error Handling Macro","text":"<p>The code defines a macro <code>handle_error(msg)</code> that is used for error handling. It prints the error message passed as an argument and exits the program if an error occurs.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#linux_dirent-structure","title":"Linux_dirent Structure","text":"<p>The code declares a structure <code>struct linux_dirent</code> to represent a directory entry. It contains fields for inode number, offset, record length, and the name of the entry.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#constant-and-buffer-size","title":"Constant and Buffer Size","text":"<p>The code defines a constant <code>BUF_SIZE</code> to determine the size of the buffer used for reading directory entries. In this case, it is set to <code>1024*1024*5</code>, indicating a buffer size of 5 MB.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#recursive-directory-listing-function","title":"Recursive Directory Listing Function","text":"<p>The <code>listdir</code> function is the core of the program and handles the recursive directory listing. It takes three parameters: <code>dname</code> (directory name), <code>otype</code> (output type), and <code>inward</code> (recursive flag).</p> <p>Inside the function, it opens the directory specified by <code>dname</code> using the <code>open</code> system call with appropriate flags. If the open operation fails, it calls the <code>handle_error</code> macro to handle the error.</p> <p>The function then enters a loop to read directory entries using the <code>getdents</code> system call. It iterates over each entry and checks its type using the <code>d_type</code> field of the struct <code>linux_dirent</code>. Based on the type and specified output type (<code>otype</code>), it prints the directory or file path.</p> <p>If the entry is a directory and the inward flag is set to 1, it recursively calls the <code>listdir</code> function with the subdirectory path.</p> <p>Finally, the function closes the directory and frees the allocated memory.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#main-function","title":"Main Function","text":"<p>The <code>main</code> function handles the command-line arguments and calls the <code>listdir</code> function.</p> <p>It uses the <code>getopt</code> function to parse the command-line options. The supported options are:</p> <ul> <li><code>-d</code>: Specifies the directory to start the listing from. If not provided, the current directory is used.</li> <li><code>-t</code>: Specifies the output type. The available options are \"files\" (only regular files), \"dirs\" (only directories), and \"dirfiles\" (both directories and files). If not provided, \"files\" is used by default.</li> <li><code>-i</code>: Specifies the inward recursion flag. If set to 1, the program will recursively list directories within directories. By default, it is set to 0 (no inward recursion). After parsing the command-line options, the <code>listdir</code> function is called with the provided arguments. The program then exits gracefully.</li> </ul>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/recursive_directory_list_in_c_using_linux_system_calls/#conclusion","title":"Conclusion","text":"<p>In this article, we explored a C code that enables recursive directory listing using Linux system calls. The code provides options to filter the output based on file types and control the depth of recursion. Understanding and utilizing such code can be beneficial when working with directory structures in a Linux environment.</p>","tags":["Linux","C","Directory Listing"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/","title":"Setup GPG Key for Github Commits","text":"<p>In today's digital world, ensuring the security and integrity of our data is of utmost importance. One way to achieve this is by using encryption and digital signatures. GnuPG (GPG) is a widely used open-source encryption software that provides cryptographic privacy and authentication for data communication. This article will guide you through the process of setting up GPG on both macOS and Linux systems.</p>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/#installation","title":"Installation","text":"<p>Install GnuPG based on Operating System</p> LinuxMAC OS Bash Session<pre><code># Install GnuPG: Most Linux distributions come with GnuPG preinstalled.\n# If it's not already installed, use your package manager to install GnuPG.\n# For example, on Ubuntu or Debian-based systems, run the following command:\nsudo apt-get install gnupg pinentry\n# In-case of Redhat-based system, run the following command:\nsudo dnf install gnupg pinentry\n</code></pre> Bash Session<pre><code># Install Homebrew: Homebrew is a popular package manager for macOS.\n# Open Terminal and run the following command to install Homebrew:\n/bin/bash -c \\\n\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install GnuPG and pinentry-mac: Once Homebrew is installed,\n# run the following command to install GnuPG and pinentry-mac:\nbrew install gnupg pinentry-mac\n</code></pre>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/#configure-gpg","title":"Configure GPG:","text":"<p>Create or modify the following configuration files to set up GPG:</p> <ul> <li><code>**~/.gnupg/gpg.conf**</code>: Open the file and add the following lines:</li> </ul> VimL<pre><code>use-agent\n# This silences the \"you need a passphrase\" message once the passphrase handling is all set.\n# Use at your own discretion - may prevent the successful interactive use of some operations.\n# It is working fine for my use cases though.\nbatch\n</code></pre> <ul> <li><code>**~/.gnupg/gpg-agent.conf**</code>: Open the file and add the following lines:</li> </ul> LinuxMAC OS VimL<pre><code># Enables GPG to find gpg-agent\nuse-standard-socket\n</code></pre> VimL<pre><code># Enables GPG to find gpg-agent\nuse-standard-socket\n\n# Connects gpg-agent to the OSX keychain via the brew-installed\n# pinentry program from GPGtools. This is the OSX 'magic sauce',\n# allowing the gpg key's passphrase to be stored in the login\n# keychain, enabling automatic key signing.\npinentry-program /opt/homebrew/bin/pinentry-mac\n</code></pre> <ul> <li>User Profile configuration for GPG</li> </ul> LinuxMAC OS <ul> <li><code>**~/.bashrc**</code> or <code>**~/.bash_profile**</code> : Open the file and add the following lines:</li> </ul> VimL<pre><code>GPG_TTY=$(tty)\nexport GPG_TTY\n</code></pre> <ul> <li><code>**~/.zprofile**</code> : Open the file and add the following lines:</li> </ul> VimL<pre><code># Add the following to your shell init to set up gpg-agent automatically for every shell\nif [ -f ~/.gnupg/.gpg-agent-info ] &amp;&amp; [ -n \"$(pgrep gpg-agent)\" ]; then\n    source ~/.gnupg/.gpg-agent-info\n    export GPG_AGENT_INFO\nelse\n    eval $(gpg-agent --daemon --write-env-file ~/.gnupg/.gpg-agent-info)\nfi\n</code></pre>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/#generate-gpg-keys","title":"Generate GPG Keys","text":"<ul> <li>Generate a key pair: Use the following command to generate your GPG key pair:</li> </ul> <p>Bash Session<pre><code>gpg --full-generate-key\n</code></pre> Follow the prompts to select the key type, key size, and expiration date. Enter a strong passphrase to protect your private key.</p> <ul> <li>Export the public key: Once the key pair is generated, you can export the public key using the key ID. Run the following command, replacing XXXXXX with your key ID:</li> </ul> Bash Session<pre><code># get the public key using key ID\ngpg --armor --export XXXXXX\n</code></pre> <p>The output will be your public key in ASCII-armored format, which can be shared with others.</p>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/#import-existing-gpg","title":"Import existing GPG","text":"<ol> <li> <p>Obtain the GPG key file: If you have the GPG key file (with a .asc or .gpg extension) from another source, make sure you have it available on your system.</p> </li> <li> <p>Import the GPG key: Open your terminal and run the following command, replacing  with the path to your GPG key file: Bash Session<pre><code>gpg --import &lt;KEY_FILE&gt;.pgp\n\n# or\n\n# Import Public key from Keybase or other server and import private key separately.\ncurl https://keybase.io/&lt;username&gt;/pgp_keys.asc | gpg --import\n</code></pre> <p>The GPG key will be imported, and you will see the key details displayed in the terminal.</p> <ol> <li>Trust the imported key (optional): By default, imported keys are not trusted. If the key belongs to someone you trust, you can manually trust it. Run the following command, replacing  with the ID of the imported key (can be found in the output of the previous command): <p>Bash Session<pre><code>gpg --edit-key &lt;KEY-ID&gt;\n</code></pre> This will open the GPG command-line interface. Enter the command trust to set the trust level for the key. Follow the prompts to select the appropriate level of trust.</p> <ol> <li>Verify the imported key: You can verify that the key has been successfully imported by running the command:</li> </ol> <p>Bash Session<pre><code>gpg --list-keys\n</code></pre> The imported key should be listed along with its details.</p>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/#configure-git-to-use-gpg","title":"Configure Git to use GPG","text":"<p>To enable Git commit signing with your GPG key, run the following command, replacing  with your key ID: Bash Session<pre><code>gpg --list-keys --keyid-format SHORT\n\ngit config --global user.signingkey &lt;YOUR-SIGNING-KEY-PUB-ID&gt;\ngit config --global commit.gpgsign true\n</code></pre> <p>Note</p> <p>SHORT keyid format will show the key-id into short format just next to rsa algorithm.</p> <p>ex:</p> <p>pub   rsa4096/<code>4BF70A73</code> 2021-09-08 [SC] [expires: 2037-09-04]</p> <p>Restart the terminal: After making these configurations, it's recommended to restart the terminal for the changes to take effect.</p> Bash Session<pre><code>## Kill gpg agent once configured\npkill -TERM gpg-agent\n</code></pre> <ul> <li>If you use Visual Studio Code, you can turn on signing by changing a setting.</li> </ul> <p>Open VSCode, go to Preferences &gt; Settings, and search for git.enableCommitSigning. Turn this setting on, and you\u2019re good to go.</p>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/setup_gpgkey_for_github_commit/#test-gpg-setup","title":"Test GPG Setup","text":"<p>To test your GPG setup, you can encrypt and decrypt a message. Run the following command, replacing  with the key ID you used: Bash Session<pre><code>echo test | gpg -e -r &lt;PUT THE KEY ID HERE&gt; | gpg -d\n</code></pre> <p>If everything is set up correctly, you should see the decrypted message \"test\" printed in the terminal.</p> <p>Congratulations! You have successfully set up GPG on your macOS or Linux system. You can now use GPG for encryption, decryption, and signing of sensitive data, providing an extra layer of security to your digital communication. Remember to keep your private key secure and never share it with anyone.</p>","tags":["gpgkey","GPG","github"]},{"location":"linux/administration/useful_commands_with_find_utility/","title":"Useful Commands Using the Find Utility in Linux","text":"","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#introduction","title":"Introduction","text":"<p>The find utility is a powerful command-line tool in Linux that allows users to search for files and directories based on various criteria. In this article, we will explore some useful commands using the find utility. These commands will help you locate specific files with alphanumeric names and special extensions, exclude certain folders or files from the search, and perform actions such as deleting or modifying files. Let's dive into the commands.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-alphanumeric-files-with-special-extension","title":"Find Alphanumeric Files with Special Extension","text":"<p>To find files with alphanumeric names and specific extensions, use the following command:</p> Bash Session<pre><code>find . -type f -regex \".*[0-9].*\\.\\(xls\\|csv\\|php\\)\"\n</code></pre> <p>This command searches for files in the current directory and its subdirectories (.) that have names containing alphanumeric characters and end with .xls, .csv, or .php extensions.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-alphanumeric-files-excluding-certain-folders","title":"Find Alphanumeric Files Excluding Certain Folders","text":"<p>If you want to exclude specific folders from the search, use the following command:</p> Bash Session<pre><code>find . -type f -regex \".*[0-9].*\\.\\(xls\\|csv\\|php\\)\" -not -path \"./database/*\"\n</code></pre> <p>This command excludes the ./database folder from the search. You can modify the path to exclude any other folders as needed.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-alphanumeric-files-excluding-multiple-folders","title":"Find Alphanumeric Files Excluding Multiple Folders","text":"<p>To skip multiple folders from the search, use the following command:</p> Bash Session<pre><code>find . -type f -regex \".*[0-9].*\\.\\(xls\\|csv\\|php\\)\" -not -path \"./database/*\" -not -path \"./public/DB/*\"\n</code></pre> <p>This command excludes both the ./database and ./public/DB folders from the search.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-alphanumeric-files-excluding-folders-and-files-with-a-pattern","title":"Find Alphanumeric Files Excluding Folders and Files with a Pattern","text":"<p>If you want to skip specific folders and files with a certain pattern, use the following command:</p> Bash Session<pre><code>find . -type f -regex \".*[0-9].*\\.\\(xls\\|csv\\|php\\)\" -not -path \"./database/*\" -not -path \"./app/Console/Commands/bulkServerUpload_[0-9].php\"\n</code></pre> <p>This command excludes the ./database folder and files matching the pattern bulkServerUpload_[0-9].php from the search.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-and-delete-dump-files","title":"Find and Delete Dump Files","text":"<p>To find dump files and delete them, use the following command:</p> Bash Session<pre><code>find . -type f -regex \".*\\.\\(zip\\|tar\\.gz\\|sql\\)\" -not -path \"./public/DB/*\" -delete\n</code></pre> <p>This command searches for files with extensions .zip, .tar.gz, or .sql and deletes them. It excludes the ./public/DB folder from the search.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-and-empty-log-files","title":"Find and Empty Log Files","text":"<p>If you want to find log files and empty their contents, use the following command:</p> Bash Session<pre><code>find . -type f -name \"*.\\(txt\\|log\\)\" -exec tee {} \\; &lt;/dev/null\n</code></pre> <p>This command finds files with extensions .txt or .log and empties their contents using the tee command.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-php-files-ending-with-whitespace","title":"Find PHP Files Ending with Whitespace","text":"<p>The first command, <code>find -name '*.php' | xargs grep -Pz '\\?&gt;[\\s]+$' -l</code>, allows you to search for PHP files that end with whitespace. Let's break down the command and understand its components.</p> Bash Session<pre><code>find -name '*.php' | xargs grep -Pz '\\?&gt;[\\s]+$' -l\n</code></pre> <ul> <li><code>find -name '*.php'</code> - This part of the command uses the find utility to locate files with the extension .php. The <code>-name</code> option specifies the pattern to match the filenames.</li> <li><code>|</code> - The pipe symbol (<code>|</code>) is used to redirect the output of the find command as input to the <code>xargs</code> command.</li> <li><code>xargs</code> - The <code>xargs</code> command reads data from standard input (in this case, the list of PHP files) and executes another command (grep) using the input as arguments.</li> <li><code>grep -Pz '\\?&gt;[\\s]+$' -l</code> - This part of the command uses grep to search for a specific pattern in the PHP files. The <code>-P</code> option enables Perl-compatible regular expressions. The <code>-z</code> option treats the input files as null-separated, allowing us to match across multiple lines. The pattern <code>\\?&gt;[\\s]+$</code> matches the closing PHP tag (<code>?&gt;</code>) followed by one or more whitespace characters at the end of the file. The <code>-l</code> option instructs <code>grep</code> to print only the filenames that match the pattern.</li> </ul> <p>When you execute this command, it will search for PHP files in the current directory and its subdirectories, filtering out those that end with whitespace.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#find-php-files-starting-with-whitespace","title":"Find PHP Files Starting with Whitespace","text":"<p>The second command, <code>find -name '*.php' | xargs grep -Pz '^[\\s]+&lt;\\?' -l</code>, helps you find PHP files that start with whitespace. Let's break down this command as well.</p> Bash Session<pre><code>find -name '*.php' | xargs grep -Pz '^[\\s]+&lt;\\?' -l\n</code></pre> <ul> <li><code>find -name '*.php'</code> - This part of the command is the same as in the previous command. It locates files with the extension .php.</li> <li><code>|</code> - The pipe symbol (<code>|</code>) redirects the output of the find command to the xargs command.</li> <li><code>xargs</code> - Again, xargs reads the list of PHP files and passes them as arguments to the next command (<code>grep</code>).</li> <li><code>grep -Pz '^[\\s]+&lt;\\?' -l</code> - This part of the command uses grep to search for PHP files that start with whitespace. The pattern <code>^[\\s]+&lt;\\?</code> matches one or more whitespace characters at the beginning of the file followed by the opening PHP tag (<code>&lt;?</code>). The <code>-l</code> option prints only the filenames that match the pattern.</li> </ul> <p>When you execute this command, it will search for PHP files in the current directory and its subdirectories, filtering out those that start with whitespace.</p>","tags":["Linux","find","commands"]},{"location":"linux/administration/useful_commands_with_find_utility/#conclusion","title":"Conclusion","text":"<p>The find utility is a versatile tool that allows you to locate files based on various criteria. By using the commands mentioned in this article, you can easily find and perform actions on files with specific characteristics, such as alphanumeric names and special extensions. Experiment with these commands and adapt them to your specific needs to efficiently manage your files in Linux.</p>","tags":["Linux","find","commands"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/","title":"Configuring MySQL Cluster with Master-Master Synchronization","text":"","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/#introduction","title":"Introduction","text":"<p>MySQL Cluster is a powerful feature in MySQL that allows for high availability and scalability by distributing the database across multiple servers. In this article, we will walk through the configuration steps for setting up a MySQL Cluster with master-master synchronization. We will assume the following server details for our setup:</p> <p>MySQL Server1: Text Only<pre><code>Server IP : 192.168.0.1\nhostname  : mysqldb1\n</code></pre> MySQL Server2:</p> Text Only<pre><code>Server IP : 192.168.0.2\nhostname  : mysqldb2\n</code></pre>","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/#configure-mysql-cluster","title":"Configure MySQL Cluster","text":"<p>To configure MySQL Server for replication, add the following configuration under the mysqld context for each MySQL Server:</p> MySQL1MySQL2 VimL<pre><code>[mysqld]\n\nbind-address        = 192.168.0.1\nserver_id           = 1\nlog_bin             = /var/log/mysql/mysql-bin.log\nlog_bin_index       = /var/log/mysql/mysql-bin.log.index\nrelay_log           = /var/log/mysql/mysql-relay-bin\nrelay_log_index     = /var/log/mysql/mysql-relay-bin.index\nexpire_logs_days    = 10\nmax_binlog_size     = 100M\nlog_slave_updates   = 1\nauto-increment-increment = 2\nauto-increment-offset = 1\n</code></pre> VimL<pre><code>[mysqld]\n\nbind-address        = 192.168.0.2\nserver_id           = 2\nlog_bin             = /var/log/mysql/mysql-bin.log\nlog_bin_index       = /var/log/mysql/mysql-bin.log.index\nrelay_log           = /var/log/mysql/mysql-relay-bin\nrelay_log_index     = /var/log/mysql/mysql-relay-bin.index\nexpire_logs_days    = 10\nmax_binlog_size     = 100M\nlog_slave_updates   = 1\nauto-increment-increment = 2\nauto-increment-offset = 2\n</code></pre>","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/#setting-up-replication","title":"Setting Up Replication","text":"<p>Next, we need to create a replica user on each server to establish replication. Run the following commands on MySQL Server1 and MySQL Server2, respectively:</p> MySQL1MySQL2 Bash Session<pre><code>mysql -uroot -p -e \"GRANT REPLICATION SLAVE ON *.* TO 'replicauser'@'192.168.0.2' IDENTIFIED BY 'replica';\"\nmysql -uroot -p -e \"FLUSH PRIVILEGES;\"\n</code></pre> Bash Session<pre><code>mysql -uroot -p -e \"GRANT REPLICATION SLAVE ON *.* TO 'replicauser'@'192.168.0.1' IDENTIFIED BY 'replica';\"\nmysql -uroot -p -e \"FLUSH PRIVILEGES;\"\n</code></pre>","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/#restarting-mariadb-service","title":"Restarting MariaDB Service","text":"<p>After creating the replica user, restart the MariaDB service on both servers to apply the configuration changes:</p> MySQL1MySQL2 Bash Session<pre><code>systemctl restart mariadb\n</code></pre> Bash Session<pre><code>systemctl restart mariadb\n</code></pre>","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/#configuring-slave-replication","title":"Configuring Slave Replication","text":"<p>Now, we need to configure slave replication on both servers. Execute the following commands on each server:</p> MySQL1MySQL2 MySQL<pre><code>STOP SLAVE;\nCHANGE MASTER TO master_host='192.168.0.2', master_port=3306, master_user='replicauser', master_password='replica', master_log_file='mysql-bin.000004', master_log_pos=3532;\nSTART SLAVE;\n</code></pre> MySQL<pre><code>STOP SLAVE;\nCHANGE MASTER TO master_host='192.168.0.1', master_port=3306, master_user='replicauser', master_password='replica', master_log_file='mysql-bin.000002', master_log_pos=531;\nSTART SLAVE;\nSHOW MASTER STATUS;\n+------------------+----------+--------------+------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |\n+------------------+----------+--------------+------------------+\n| mysql-bin.000004 |     3532 |              |                  |\n+------------------+----------+--------------+------------------+\n</code></pre>","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/configure_mysql_cluster_master_master_synchronization/#data-synchronization","title":"Data Synchronization","text":"<p>With the configuration and replication set up, any data created on either MariaDB server (MySQL Server1 or MySQL Server2) will be synchronized accordingly. The master-master synchronization ensures that changes made on one server are replicated to the other, allowing for high availability and data redundancy in the MySQL Cluster.</p> <p>In this article, we have covered the essential steps to configure a MySQL Cluster with master-master synchronization. By following these steps, you can create a robust and scalable database environment for your applications.</p>","tags":["High Availability","Cluster","MySQL"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/","title":"MySQL High Availability with Keepalived and HAProxy","text":"","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#introduction","title":"Introduction","text":"<p>Ensuring high availability and load balancing for MySQL is essential for maintaining a reliable and scalable database infrastructure. In this article, we will explore how to achieve high availability and load balancing for MySQL using Keepalived and HAProxy. Keepalived is a high availability monitor, while HAProxy acts as a load balancer. Together, they provide a robust solution for creating a high availability cluster for MySQL or any other application service.</p> <p>We will assume the following configuration details for our setup:</p> <ul> <li>Virtual IP: 192.168.0.100</li> <li>Load Balancer Server 1 (loadb1): 192.168.0.25</li> <li>Load Balancer Server 2 (loadb2): 192.168.0.26 (for failover)</li> <li>App Server 1 (appserver1): 192.168.0.1</li> <li>App Server 2 (appserver2): 192.168.0.2</li> </ul>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#keepalived","title":"Keepalived","text":"<p>Keepalived is a Linux Virtual Server that balances IP load across a set of real servers. It operates on a pair of equally configured computers: an active LVS router and a backup LVS router. Additionally, Keepalived uses Virtual Router Redundancy Protocol (VRRP) to ensure network connectivity even if the default router fails.</p>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#keepalived-configuration","title":"Keepalived Configuration","text":"<ol> <li>Install the Keepalived package on the load balancing server (loadb1), where HAProxy will run:     Bash Session<pre><code>yum install keepalived\n</code></pre></li> <li> <p>Backup the existing Keepalived configuration:</p> Bash Session<pre><code>mv /etc/keepalived/keepalived.conf{,.back}\nvim /etc/keepalived/keepalived.conf\n</code></pre> </li> <li> <p>Configure the Keepalived using the following example configuration, based on your server's IP configuration:</p> <p>VimL<pre><code>global_defs {\n    notification_email {\n        user@mydomain.com\n    }\n    notification_email_from noreply@mydomain.com\n    smtp_server 127.0.0.1\n    smtp_connect_timeout 30\n}\nvrrp_instance VRRP1 {\n    state MASTER\n    interface eth0\n    virtual_router_id 71\n    priority 200\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1234\n    }\n    virtual_ipaddress {\n        192.168.0.100\n    }\n}\n</code></pre> 4. Edit the /etc/sysctl.conf file to enable the non-local IP binding:</p> <p>Bash Session<pre><code>vi /etc/sysctl.conf\n</code></pre>   Add the following line:</p> Bash Session<pre><code>net.ipv4.ip_nonlocal_bind=1\n</code></pre> </li> <li> <p>Restart the Keepalived daemon and enable it to start on boot:</p> <p>Bash Session<pre><code>systemctl restart keepalived\nsystemctl enable keepalived\n</code></pre>   Now, the load balancer server (loadb1) will have the virtual IP address assigned to its physical network interface.</p> </li> </ol>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#backup-load-balancer-failover","title":"Backup Load Balancer (Failover)","text":"<p>To configure a backup load balancer server (loadb2) for failover, follow the same steps as above but use the following configuration</p> VimL<pre><code>global_defs {\n    notification_email {\n        user@mydomain.com\n    }\n    notification_email_from noreply@mydomain.com\n    smtp_server 127.0.0.1\n    smtp_connect_timeout 30\n}\nvrrp_instance VRRP1 {\n    state BACKUP\n    interface eth0\n    virtual_router_id 71\n    priority 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1234\n    }\n    virtual_ipaddress {\n        192.168.0.100\n    }\n}\n</code></pre> <p>The backup load balancer server (loadb2) will assign the virtual IP address to its network interface only when the master load balancer becomes unreachable.</p>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#haproxy","title":"HAProxy","text":"<p>HAProxy is a high availability load balancer that ensures efficient distribution of traffic among multiple servers. It is installed on the load balancing servers (loadb1 and loadb2).</p>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#haproxy-configuration","title":"HAProxy Configuration","text":"<ol> <li> <p>Install the HAProxy package:</p> Bash Session<pre><code>yum install haproxy\n</code></pre> </li> <li> <p>Backup the default HAProxy configuration:</p> Bash Session<pre><code>mv /etc/haproxy/haproxy.cfg{,.back}\n</code></pre> </li> <li> <p>Create a new HAProxy configuration file with the following content:</p> VimL<pre><code>global\n    user haproxy\n    group haproxy\n\ndefaults\n    mode http\n    log global\n    retries 2\n    timeout connect 3000ms\n    timeout server 5000ms\n    timeout client 5000ms\n\nlisten stats\n    bind 192.168.0.100:9999\n    stats enable\n    stats hide-version\n    stats uri /stats\n    stats auth statadmin:statadminpass\n\nlisten mysql-cluster\n    bind 192.168.0.100:3306\n    mode tcp\n    option mysql-check user haproxy_check\n    balance roundrobin\n    server mysql-1 192.168.0.1:3306 check\n    server mysql-2 192.168.0.2:3306 check\n</code></pre> </li> <li> <p>Before starting HAProxy, create a user on each MySQL server for health checks. This user should be created without a password</p> Bash Session<pre><code>mysql -u root -p\nMariaDB&gt; CREATE USER 'haproxy_check'@'%';\nMariaDB&gt; FLUSH PRIVILEGES;\n</code></pre> </li> <li> <p>Restart the HAProxy service and enable it to start on boot:</p> Bash Session<pre><code>systemctl restart haproxy\nsystemctl enable haproxy\n</code></pre> </li> </ol> <p>Now, HAProxy is configured to balance the traffic across the MySQL servers (appserver1 and appserver2) using the virtual IP address.</p>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/mysql/mysql_high_availability_with_keepalived_and_haproxy/#conclusion","title":"Conclusion","text":"<p>In this article, we have explored how to achieve high availability and load balancing for MySQL using Keepalived and HAProxy. By configuring Keepalived to monitor and manage the virtual IP address, and setting up HAProxy as a load balancer, we can ensure a reliable and scalable MySQL cluster. This setup provides failover capabilities and efficient distribution of traffic, resulting in improved availability and performance for your MySQL-based applications.</p>","tags":["High Availability","Cluster","MySQL","Keepalived","HAProxy"]},{"location":"linux/overview/booting_process_of_computer_system/","title":"Understanding the Booting Process of a Computer System","text":"<p>Booting a computer system involves a sequence of processes performed by both the hardware and software components. This article will guide you through the step-by-step process of booting, starting from the power-on stage to when the operating system is fully loaded and ready for use. Let's explore each stage in detail.</p> <ul> <li>Power Supply</li> <li>Basic Input Output System -BIOS</li> <li>Booting Record/Partition Table</li> <li>Bootloader - GRUB</li> <li>Kernel</li> <li>Init</li> <li>RunLevel Programs</li> </ul>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#power-supply","title":"Power Supply","text":"<p>When you turn on your computer, the power supply unit (PSU) undergoes a self-test and sends a power signal to the CPU motherboard and other related components if the self-test is successful. If the self-test fails, there will be no output.</p>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#basic-input-output-system-bios","title":"Basic Input Output System - BIOS","text":"<p>The BIOS, an interface programmed into an integrated circuit chip, plays a crucial role in obtaining hardware information from your computer. It also includes CMOS (Complementary Metal Oxide Semiconductor), a type of storage that holds essential hardware settings required by the operating system, such as drive locations, configurations, memory speed, CPU frequency multiplier, and more. Additionally, BIOS performs a power-on self-test during system startup. Here's a checklist of what BIOS verifies to continue the booting process:</p> <ul> <li>Checks if all available hard drives and devices are responding.</li> <li>Ensures that the mouse and keyboard are connected.</li> <li>Checks the availability and functionality of the video card.</li> <li>Initializes the RAID controller if installed.</li> </ul> <p>BIOS also stores the booting sequence information, which specifies the order in which the system searches for the booting device.</p>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#booting-recordpartition-table","title":"Booting Record/Partition Table","text":"<p>Once BIOS identifies the boot partition, it searches for the booting records, which consist of the partition table and boot loader information.</p>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#boot-loader-information","title":"Boot Loader Information","text":"<ul> <li>The boot loader information block contains the first program that the computer can run.</li> </ul>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#disk-partition-table","title":"Disk partition Table","text":"<ul> <li> <p>The partition table stores information about how the disk drive is logically organized.     There are two types of Booting Record are available.</p> <ol> <li> <p>Master Boot Record (MBR): This is the older method of partition table storage, typically located in the first sector of the drive. It occupies only the first 512 bytes of space, with the first 448 bytes allocated for boot loader information and the remaining 64 bytes for the disk partition table. Each partition occupies 16 bytes within these 64 bytes, and the remaining 2 bytes serve as a magic number for bootloader validation.</p> </li> <li> <p>GUID Partition Table (GPT): GPT is the newer method of partition table storage and is usually present in both the first and last sectors of the drive. If the first sector is corrupted, the system can boot from the backup located in the last sector. GPT uses globally unique IDs (GUIDs) to identify partitions instead of partition numbers used in MBR. Unlike MBR, GPT has no limitation on the number of partitions and supports partitions larger than 2TB.</p> </li> </ol> </li> </ul>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#boot-loader","title":"Boot Loader","text":"<p>The boot loader's role is to load the kernel and its supporting modules into memory. One commonly used boot loader is GRUB (Grand Unified Bootloader), which is widely adopted across various Linux distributions. Another boot loader, Linux Loader (LILO), is an older alternative.</p> <p>GRUB has knowledge of different filesystems and can understand the partition filesystem type, while LILO does not have this capability.</p> <p>By defining a configuration file, GRUB waits for the user to select the desired kernel image from the list of installed kernels. If no selection is made, it loads the default kernel image.</p>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#kernel","title":"Kernel","text":"<p>The kernel mounts the filesystem specified in the boot loader's configuration file (grub.conf) and executes the init program. The init program, with a ProcessID (PID) of 1, is the first program launched during system startup.</p> <p>During kernel boot, the initrd (initial RAM disk) is loaded into memory, serving as a temporary root file system. This allows the kernel to fully boot without mounting any physical disks.</p> <p>The kernel itself can be relatively small while still supporting a wide range of hardware configurations. Once the kernel is booted, the root file system is pivoted (via pivot_root), unmounting the initrd root file system and mounting the real root file system.</p>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#init","title":"Init","text":"<p>The init program determines the default init level from the inittab file and uses it to load all the appropriate programs.</p> <p>The default init levels are defined by the initdefault setting in the /etc/inittab file. The following are the boot system init levels and their corresponding functions:</p> <ul> <li>0: Halt</li> <li>1: Single User Mode</li> <li>2: Local MultiUser without networking/NFS</li> <li>3: Full MultiUser with Networking</li> <li>4: Not Used (User-defined)</li> <li>5: Graphical Mode (with X11 System)</li> <li>6: Reboot</li> </ul>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#run-level-programs","title":"Run Level Programs","text":"<p>Run level programs are responsible for executing the required programs based on the system's current run level, as determined by the init program.</p> <p>These programs start or kill services depending on the run level, and their configurations are stored in /etc/rc.d/rcX.d/ directories, where X represents the init level.</p> <p>For example:</p> <ul> <li>S80sendmail - /etc/rc.d/rc3.d/   This means that the sendmail service should start (S) during system boot at init level 3, with a sequence number of 80.</li> <li>K12syslog - /etc/rc.d/rc5.d/   This means that the syslog service should be stopped (K) during system boot at init level 5, with a sequence number of 12.</li> </ul>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/booting_process_of_computer_system/#conclusion","title":"Conclusion:","text":"<p>Understanding the booting process is essential for troubleshooting and configuring computer systems. By following the step-by-step sequence described in this article, you can gain a better understanding of how a computer boots and loads the operating system.</p>","tags":["Disambiguation","Learn","Linux","booting process","booting"]},{"location":"linux/overview/filesystem_overview_functionality/","title":"Understanding File Systems: A Comprehensive Overview and Functionality","text":"","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#what-is-file-system","title":"What is File System","text":"<p>File System is a methods and structure of data that an operating system used to keep track the files and control the data are organized in the disk or partition.</p> <p>File system logically defined the files in the disk by mentioned below :</p> <ul> <li>Sharable - It can be accessed locally or by remote host.</li> <li>unsharable - Only It can be accessed locally.</li> <li>Variable - These files can able to change at any time such as documents.</li> <li>Static - Static Binaries which is do not change without administrator permission.It used to execute the program to the user.</li> </ul>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#types-of-file-systems","title":"Types of File systems","text":"","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#disk-file-systems","title":"Disk File Systems","text":"<p>Ability of disk storage media to randomly address data in a short amount of time.Additional considerations include the speed of accessing data following that initially requested and the anticipation that the following data may also be requested. This permits multiple users (or processes) access to various data on the disk without regard to the sequential location of the data. Some disk file systems are journaling file systems or versioning file systems.</p> <p>Here are some of Disk File Systems are :</p> <ul> <li>FAT (FAT12, FAT16, FAT32),</li> <li>exFAT,</li> <li>NTFS,</li> <li>HFS and HFS+, HPFS,</li> <li>UFS,</li> <li>ext2, ext3, ext4,</li> <li>XFS,</li> <li>btrfs,</li> <li>ISO 9660,</li> <li>Files-11,</li> <li>Veritas File System,</li> <li>VMFS,</li> <li>ZFS,</li> <li>ReiserFS and UDF.</li> </ul> <p>ISO 9660 and Universal Disk Format (UDF) are two common formats that target Compact Discs, DVDs and Blu-ray discs.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#flash-file-systems","title":"Flash File Systems","text":"<p>A flash file system considers the special abilities, performance and restrictions of flash memory devices. Frequently a disk file system can use a flash memory device as the underlying storage media but it is much better to use a file system specifically designed for a flash device.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#tape-file-systems","title":"Tape File Systems","text":"<p>A tape file system is a file system and tape format designed to store files on tape in a self-describing form. Magnetic tapes are sequential storage media with significantly longer random data access times than disks, posing challenges to the creation and efficient management of a general-purpose file system.</p> <p>In a disk file system there is typically a master file directory, and a map of used and free data regions. Any file additions, changes, or removals require updating the directory and the used/free maps. Random access to data regions is measured in milliseconds so this system works well for disks.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#database-file-systems","title":"Database file systems","text":"<p>Another concept for file management is the idea of a database-based file system. Instead of, or in addition to, hierarchical structured management, files are identified by their characteristics, like type of file, topic, author, or similar rich metadata.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#transactional-file-systems","title":"Transactional file systems","text":"<p>Some programs need to update multiple files \"all at once\". For example, a software installation may write program binaries, libraries, and configuration files. If the software installation fails, the program may be unusable. If the installation is upgrading a key system utility, such as the command shell, the entire system may be left in an unusable state.</p> <p>Transaction processing introduces the isolation guarantee, which states that operations within a transaction are hidden from other threads on the system until the transaction commits, and that interfering operations on the system will be properly serialized with the transaction. Transactions also provide the atomicity guarantee, ensuring that operations inside of a transaction are either all committed or the transaction can be aborted and the system discards all of its partial results. This means that if there is a crash or power failure, after recovery, the stored state will be consistent. Either the software will be completely installed or the failed installation will be completely rolled back, but an unusable partial install will not be left on the system.</p> <p>Journaling file systems are one technique used to introduce transaction-level consistency to file system structures. Journal transactions are not exposed to programs as part of the OS API; they are only used internally to ensure consistency at the granularity of a single system call.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#network-file-systems","title":"Network File Systems","text":"<p>A network file system is a file system that acts as a client for a remote file access protocol, providing access to files on a server. Programs using local interfaces can transparently create, manage and access hierarchical directories and files in remote network-connected computers.</p> <p>Examples of network file systems include clients for the</p> <ul> <li>NFS,</li> <li>AFS,</li> <li>SMB protocols,</li> </ul> <p>And file-system-like clients for</p> <ul> <li>FTP</li> <li>WebDAV.</li> </ul>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#shared-disk-file-systems","title":"Shared disk file systems","text":"<p>A shared disk file system is one in which a number of machines (usually servers) all have access to the same external disk subsystem (usually a SAN). The file system arbitrates access to that subsystem, preventing write collisions.</p> <p>Examples include</p> <ul> <li>GFS2 from Red Hat,</li> <li>GPFS from IBM,</li> <li>SFS from DataPlow,</li> <li>CXFS from SGI and</li> <li>StorNext from Quantum Corporation.</li> </ul>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#special-file-systems","title":"Special file systems","text":"<p>A special file system presents non-file elements of an operating system as files so they can be acted on using file system APIs. This is most commonly done in Unix-like operating systems, but devices are given file names in some non-Unix-like operating systems as well.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#device-file-systems","title":"Device file systems","text":"<p>A device file system represents I/O devices and pseudo-devices as files, called device files. Examples in Unix-like systems include devfs and, in Linux 2.6 systems, udev. In non-Unix-like systems, such as TOPS-10 and other operating systems influenced by it, where the full filename or pathname of a file can include a device prefix, devices other than those containing file systems are referred to by a device prefix specifying the device, without anything following it.</p>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#other-special-file-systems","title":"Other special file systems","text":"<ul> <li> <p>In the Linux kernel, configfs and sysfs provide files that can be used to query the kernel for information and configure entities in the kernel.</p> </li> <li> <p>procfs maps processes and, on Linux, other operating system structures into a filespace.</p> </li> </ul>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#linux-file-system-hierarchy","title":"Linux File System Hierarchy","text":"<ul> <li> <p>/boot/ - Directory contains static files required to boot the system such as kernel.</p> </li> <li> <p>/dev/ - Directory contains the device nodes that either represent devices that are attached to the system or virtual devices that are provided by the kernel.The udev dameon takes care of creating and removing all these device node on this directory.</p> </li> <li> <p>/etc/ - Reserved for configuration files that are required to the local machine.</p> </li> <li> <p>/lib/ - Directory should contain the libraries needed to execute binaries are located on /bin/ and /sbin/. It is necessary to executes the commands within the root file system.</p> </li> <li> <p>/media/ - Directory should used as mount point for removal media such as DVD-ROM or USB storage media stick.</p> </li> <li> <p>/mnt/ - Directory reserved for temporary mounted file systems.</p> </li> <li> <p>/opt/ - Directory to hold storage for most of application packages.It hold the manual files,binaries etc,.</p> </li> <li> <p>/proc/ - Contains special files that either extract information from or send information to the kernel.for example it contain system memory,cpu information and other hardware information etc.</p> </li> <li> <p>/bin/ - All the executable binary programs (file) required during booting, repairing, files required to run into single-user-mode, and other important, basic commands viz., cat, du, df, tar, rpm, wc, history, etc.</p> </li> <li> <p>/sbin/ - Contains System binaries by the root user.It is used at boot time. It is essential for booting,restoring,recovering,repairing the system.</p> </li> <li> <p>/srv/ - Directory contains site-specific data served by your system.It gives users the location of data files for particular service.</p> </li> <li> <p>/sys/ - It contains the information about device similarly held in /proc/ directory. and also it utilizes the new systfs virtual file system specific to the 2.6 kernel.</p> </li> <li> <p>/usr/ - Directory for the user level accessing files for configuration,binaries,libraries.And it contain the files that can shared across multiple machines.</p> </li> <li> <p>/var/ - Directory for any programs to write logs or any other data need to stored by the programs.</p> </li> </ul>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#filesystem-comparison","title":"Filesystem comparison","text":"FS Name Year Introduced Original OS Max File Size Max FS Size Journaling FAT16 1983 MSDOS V2 4GB 16MB to 8GB N FAT32 1997 Windows 95 4GB 8GB to 2TB N HPFS 1988 OS/2 4GB 2TB N NTFS 1993 Windows NT 16EB 16EB Y HFS+ 1998 Mac OS 8EB ? N UFS2 2002 FreeBSD 512GB to 32PB 1YB N ext2 1993 Linux 16GB to 2TB4 2TB to 32TB N ext3 1999 Linux 16GB to 2TB4 2TB to 32TB Y ReiserFS3 2001 Linux 8TB8 16TB Y ReiserFS4 2005 Linux ? ? Y XFS 1994 IRIX 9EB 9EB Y JFS ? AIX 8EB 512TB to 4PB Y VxFS 1991 SVR4.0 16EB ? Y ZFS 2004 Solaris 10 1YB 16EB N","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#sizes-table","title":"Sizes Table","text":"Size Equivant Size Kilobyte - KB 1024 Bytes Megabyte - MB 1024 KBs Gigabyte - GB 1024 MBs Terabyte - TB 1024 GBs Petabyte - PB 1024 TBs Exabyte - EB 1024 PBs Zettabyte - ZB 1024 EBs Yottabyte - YB 1024 ZBs","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#creating-a-filesystem","title":"Creating a FileSystem","text":"<p>Before creating a filesystem make sure the disk which you need to create is already unmounted or not.If not then unmount the same.And kindly note down creating filesystem would lost of entire data on the disk.</p> <ul> <li>fdisk -l</li> </ul> <p>It would list down all the available devices on the system.get the disk partition name from this command.</p> <ul> <li>umount /dev/sdb1</li> </ul> <p>It should the un mount the hard disk from the system.</p> <ul> <li>mkfs.vfat /dev/sdb1 -n DiskName</li> </ul> <p>It should format the disk partition which is specified and make the filesystem.</p> <ol> <li> <p>mkfs - mkfs is used to build a Linux filesystem on a device, usually a hard disk partition. The device argument is either the device name (e.g. /dev/hda1, /dev/sdb2), or a regular file that shall contain the filesystem. The size argument is the number of blocks to be used for the filesystem.</p> </li> <li> <p>vfat - Formats the drive to FAT32 as filesystem type.And other formats available are</p> <p>mkfs.bfs, mkfs.ext2, mkfs.ext3, mkfs.ext4, mkfs.minix, mkfs.msdos, mkfs.vfat, mkfs.xfs, mkfs.xiafs etc</p> </li> <li> <p>/dev/sdb1 - Disk partition name on the disk should going to build the filesystem.</p> </li> <li> <p><code>-n</code> - Sets the volume name (label) of the filesystem. The volume name can be up to 11 characters long. The default is no label.DiskName is the name which you required for the filesystem.</p> </li> <li> <p>Optional Field.And it may vary depend upon the Filesystem.For Example Label option should be -L for NTFS Filesystem.</p> </li> </ol>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/filesystem_overview_functionality/#important-files-and-usage","title":"Important Files and Usage","text":"<ul> <li> <p>/boot/vmlinuz : The Linux Kernel file.</p> </li> <li> <p>/dev/hda : Device file for the first IDE HDD (Hard Disk Drive)</p> </li> <li> <p>/dev/hdc : Device file for the IDE Cdrom, commonly</p> </li> <li> <p>/dev/null : A pseudo device, that don\u2019t exist. Sometime garbage output is redirected to /dev/null, so that it gets lost, forever.</p> </li> <li> <p>/etc/bashrc : Contains system defaults and aliases used by bash shell.</p> </li> <li> <p>/etc/crontab : A shell script to run specified commands on a predefined time Interval.</p> </li> <li> <p>/etc/exports : Information of the file system available on network.</p> </li> <li> <p>/etc/fstab : Information of Disk Drive and their mount point.</p> </li> <li> <p>/etc/group : Information of Security Group.</p> </li> <li> <p>/etc/grub.conf : grub bootloader configuration file.</p> </li> <li> <p>/etc/init.d : Service startup Script.</p> </li> <li> <p>/etc/lilo.conf : lilo bootloader configuration file.</p> </li> <li> <p>/etc/hosts : Information of Ip addresses and corresponding host names.</p> </li> <li> <p>/etc/hosts.allow : List of hosts allowed to access services on the local machine.</p> </li> <li> <p>/etc/host.deny : List of hosts denied to access services on the local machine.</p> </li> <li> <p>/etc/inittab : INIT process and their interaction at various run level.</p> </li> <li> <p>/etc/issue : Allows to edit the pre-login message.</p> </li> <li> <p>/etc/modules.conf : Configuration files for system modules.</p> </li> <li> <p>/etc/motd : motd stands for Message Of The Day, The Message users gets upon login.</p> </li> <li> <p>/etc/mtab : Currently mounted blocks information.</p> </li> <li> <p>/etc/passwd : Contains password of system users in a shadow file, a security implementation.</p> </li> <li> <p>/etc/printcap : Printer Information</p> </li> <li> <p>/etc/profile : Bash shell defaults</p> </li> <li> <p>/etc/profile.d : Application script, executed after login.</p> </li> <li> <p>/etc/rc.d : Information about run level specific script.</p> </li> <li> <p>/etc/rc.d/init.d : Run Level Initialisation Script.</p> </li> <li> <p>/etc/resolv.conf : Domain Name Servers (DNS) being used by System.</p> </li> <li> <p>/etc/securetty : Terminal List, where root login is possible.</p> </li> <li> <p>/etc/skel : Script that populates new user home directory.</p> </li> <li> <p>/etc/termcap : An ASCII file that defines the behaviour of Terminal, console and printers.</p> </li> <li> <p>/etc/X11 : Configuration files of X-window System.</p> </li> <li> <p>/usr/bin : Normal user executable commands.</p> </li> <li> <p>/usr/bin/X11 : Binaries of X windows System.</p> </li> <li> <p>/usr/include : Contains include files used by \u2018\\ c\\ \u2018 program.</p> </li> <li> <p>/usr/share : Shared directories of man files, info files,etc.</p> </li> <li> <p>/usr/lib : Library files which are required during program compilation.</p> </li> <li> <p>/usr/sbin : Commands for Super User, for System Administration.</p> </li> <li> <p>/proc/cpuinfo : CPU Information</p> </li> <li> <p>/proc/filesystems : File-system Information being used currently.</p> </li> <li> <p>/proc/interrupts : Information about the current interrupts being utilised currently.</p> </li> <li> <p>/proc/ioports : Contains all the Input/Output addresses used by devices on the server.</p> </li> <li> <p>/proc/meminfo : Memory Usages Information.</p> </li> <li> <p>/proc/modules : Currently using kernel module.</p> </li> <li> <p>/proc/mount : Mounted File-system Information.</p> </li> <li> <p>/proc/stat : Detailed Statistics of the current System.</p> </li> <li> <p>/proc/swaps : Swap File Information.</p> </li> <li> <p>/version : Linux Version Information.</p> </li> <li> <p>/var/log/lastlog : log of last boot process.</p> </li> <li> <p>/var/log/messages : log of messages produced by syslog daemon at boot.</p> </li> <li> <p>/var/log/wtmp : list login time and duration of each user on the system currently.</p> </li> </ul>","tags":["Linux","Learn","Linux filesystem","linux file tree","filesystem"]},{"location":"linux/overview/introduction_about_computer_network/","title":"What is Computer Network ?.Introduction About Computer Network","text":"<p>\"A Computer network or data network is a telecommunications network that allows computers to exchange data. In computer networks, networked computing devices pass data to each other along different data connections. Data is transferred in the form of packets by using encoding and decoding standards.\"</p> <p>Network computer devices that originate, route and terminate the data are called network nodes. Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices are said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other.</p> <p>Computer networks support applications such as access to the World Wide Web, shared use of application and storage servers,printers, and fax machines, and use of email and instant messaging applications. Computer networks differ in the physical media used to transmit their signals, the communications protocols to organize network traffic, the network's size, topology and organizational intent.</p> <p>A computer network facilitates interpersonal communications allowing people to communicate efficiently and easily via email, instant messaging, chat rooms, telephone, video telephone calls, and video conferencing. Providing access to information on shared storage devices is an important feature of many networks. A network allows sharing of files, data, devices and other types of information giving authorized users the ability to access information stored on other computers on the network. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer.Distributed computing uses computing resources across a network to accomplish tasks. A computer network may be used by computer Crackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network (denial of service). A complex computer network may be difficult to set up. It may be costly to set up an effective computer network in a large organization.</p> <p>Computer Network formed by using network topology that connects network nodes to transmit the network packet effectively in a complex networking.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#network-packet","title":"Network packet","text":"<p>A network packet is a formatted unit of data (a list of bits or bytes) carried by a packet-switched network. Computer communications links that do not support packets, such as traditional point-to-point telecommunications links, simply transmit data as a bit stream. When data is formatted into packets, the bandwidth of the communication medium can be better shared among users than if the network were circuit switched.</p> <p>A packet consists of two kinds of data: control information and user data (also known as payload). The control information provides data the network needs to deliver the user data, for example: source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#network-topology","title":"Network topology","text":"<p>Network topology is the arrangement of the various elements (links, nodes, etc.) of a computer network.Essentially, it is the topological structure of a network and may be depicted physically or logically. Physical topology is the placement of the various components of a network, including device location and cable installation, while logical topology illustrates how data flows within a network, regardless of its physical design. Distances between nodes, physical interconnections, transmission rates, or signal types may differ between two networks, yet their topologies may be identical.</p> <p>An example is a local area network (LAN): Any given node in the LAN has one or more physical links to other devices in the network; graphically mapping these links results in a geometric shape that can be used to describe the physical topology of the network. Conversely, mapping the data flow between the components determines the logical topology of the network.</p> <p>Common Topologies are:</p> <ul> <li>A bus topology: all nodes are connected to a common medium along this medium. This was the layout used in the originalEthernet, called 10BASE5 and 10BASE2.</li> <li>A star topology: all nodes are connected to a special central node. This is the typical layout found in a Wireless LAN, where each wireless client connects to the central Wireless access point.</li> <li>A ring topology: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The Fiber Distributed Data Interface (FDDI) made use of such a topology.</li> <li>A mesh topology: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other.</li> <li>A fully connected topology: each node is connected to every other node in the network.</li> <li>A tree topology: nodes are arranged hierarchically.</li> </ul>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#network-nodes","title":"Network nodes","text":"<p>In communication networks, a node is either a connection point, a redistribution point or a communication endpoint (some terminal equipment). The definition of a node depends on the network and protocol layer referred to. A physical network node is an active electronic device that is attached to a network, and is capable of sending, receiving, or forwarding information over a communications channel. A passive distribution point such as a distribution frame or patch panel is consequently not a node.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#network-interfaces","title":"Network interfaces","text":"<p>An ATM network interface in the form of an accessory card. A lot of network interfaces are built-in.</p> <p>A network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.</p> <p>The NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.</p> <p>In Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address-usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#repeaters-and-hubs","title":"Repeaters and hubs","text":"<p>A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise, and regenerates it. The signal is retransmitted at a higher power level, or to the other side of an obstruction, so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.</p> <p>A repeater with multiple ports is known as a hub. Repeaters work on the physical layer of the OSI model. Repeaters require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance. As a result, many network architectures limit the number of repeaters that can be used in a row, e.g., the Ethernet 5-4-3 rule.</p> <p>Hubs have been mostly obsoleted by modern switches; but repeaters are used for long distance links, notably undersea cabling.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#bridges","title":"Bridges","text":"<p>A network bridge connects and filters traffic between two network segments at the data link layer (layer 2) of the OSI model to form a single network. This breaks the network's collision domain but maintains a unified broadcast domain. Network segmentation breaks down a large, congested network into an aggregation of smaller, more efficient networks.</p> <p>Bridges come in three basic types:</p> <ul> <li>Local bridges: Directly connect LANs</li> <li>Remote bridges: Can be used to create a wide area network (WAN) link between LANs. Remote bridges, where the connecting link is slower than the end networks, largely have been replaced with routers.</li> <li>Wireless bridges: Can be used to join LANs or connect remote devices to LANs.</li> </ul>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#switches","title":"Switches","text":"<p>A network switch is a device that forwards and filters OSI layer 2 datagrams between ports based on the MAC addresses in the packets.[8] A switch is distinct from a hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected. It can be thought of as a multi-port bridge.[9] It learns to associate physical ports to MAC addresses by examining the source addresses of received frames. If an unknown destination is targeted, the switch broadcasts to all ports but the source. Switches normally have numerous ports, facilitating a star topology for devices, and cascading additional switches.</p> <p>Multi-layer switches are capable of routing based on layer 3 addressing or additional logical levels. The term switch is often used loosely to include devices such as routers and bridges, as well as devices that may distribute traffic based on load or based on application content (e.g., a Web URL identifier).</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#routers","title":"Routers","text":"<p>A typical home or small office router showing the ADSL telephone line and Ethernet network cable connections</p> <p>A router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram (Internet protocol information from layer 3). The routing information is often processed in conjunction with the routing table (or forwarding table). A router uses its routing table to determine where to forward packets. (A destination in a routing table can include a \"null\" interface, also known as the \"black hole\" interface because data can go into it, however, no further processing is done for said data.)</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#modems","title":"Modems","text":"<p>Modems (MOdulator-DEModulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more frequencies are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Modems are commonly used for telephone lines, using a Digital Subscriber Line technology.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/introduction_about_computer_network/#firewalls","title":"Firewalls","text":"<p>A firewall is a network device for controlling network security and access rules. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.</p>","tags":["Disambiguation","Learn","Linux","notes about network","computer network","about network","Network"]},{"location":"linux/overview/network_file_system_overview_and_configuration/","title":"Network file system overview and configuration","text":"<ul> <li>NFS</li> <li>Network File System</li> <li>File Share</li> <li>Network File Share</li> </ul>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#what-is-nfs","title":"What is NFS ?","text":"<p>Network File System (NFS) is a distributed file system,lets a computer user to access the files on a remote computer as though they were on the user's own computer.Depending upon the configuration files are accessible over the network and allow mounting the distributed file system on remote hosts. on  The user's system needs to have an NFS client and the other computer needs the NFS server. Both of them require that you also have TCP/IP installed since the NFS server and client use TCP/IP as the program that sends the files and updates back and forth. (However, the User Datagram Protocol, UDP, which comes with TCP/IP, is used instead of TCP with earlier versions of NFS.)</p>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#required-services-for-nfs","title":"Required Services for NFS","text":"Services Description Port Number Transport nfs NFS Server- Shared file System 2049 TCP/UDP nfslock NFS Lock Manager-Lock files on the server. 32803 (Optional) TCP/UDP portmapper Accepts Port reservation for RPC Services 111 TCP/UDP <p>And following RPC Services facilities the NFS Services.</p> <ul> <li> <p>rpc.mountd \u2014 This process receives mount requests from NFS clients and verifies the requested file system is currently exported. This process is started automatically by the nfs service and does not require user configuration.</p> </li> <li> <p>rpc.nfsd \u2014 Allows explicit NFS versions and protocols the server advertises to be defined. It works with the Linux kernel to meet the dynamic demands of NFS clients, such as providing server threads each time an NFS client connects. This process corresponds to the nfs service.</p> </li> <li> <p>rpc.lockd  \u2014 allows NFS clients to lock files on the server. If rpc.lockd is not started, file locking will fail. rpc.lockd implements the Network Lock Manager (NLM) protocol. This process corresponds to the nfslock service. This is not used with NFSv4.</p> </li> <li> <p>rpc.statd \u2014 This process implements the Network Status Monitor (NSM) RPC protocol which notifies NFS clients when an NFS server is restarted without being gracefully brought down. This process is started automatically by the nfslock service and does not require user configuration. This is not used with NFSv4.</p> </li> <li> <p>rpc.rquotad \u2014 This process provides user quota information for remote users. This process is started automatically by the nfs service and does not require user configuration.</p> </li> <li> <p>rpc.idmapd \u2014 This process provides NFSv4 client and server upcalls which map between on-the-wire NFSv4 names (which are strings in the form of user@domain) and local UIDs and GIDs. For idmapd to function with NFSv4, the /etc/idmapd.conf must be configured. This service is required for use with NFSv4.</p> </li> </ul>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#installation-of-nfs","title":"Installation of NFS","text":""},{"location":"linux/overview/network_file_system_overview_and_configuration/#server-installation-of-nfs","title":"Server Installation of NFS","text":"<p>In Server side we need to install nfs-kernel-server and nfs-common packages need to install for further configuration and using the NFS service.</p> Ubuntu/DebianCentOS/RHEL Bash Session<pre><code>apt-get install nfs-kernel-server\n\napt-get install nfs-common\n</code></pre> Bash Session<pre><code>yum install nfs-utils nfs-utils-lib\n</code></pre>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#client-installation-of-nfs","title":"Client Installation of NFS","text":"<p>In client side we need an nfs-common package required to using the NFS Service.</p> Ubuntu/DebianCentOS/RHEL Bash Session<pre><code>apt-get install nfs-common\n</code></pre> Bash Session<pre><code>yum install nfs-utils nfs-utils-lib\n</code></pre>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#configuration-of-nfs","title":"Configuration of NFS","text":""},{"location":"linux/overview/network_file_system_overview_and_configuration/#server-configuration-of-nfs","title":"Server Configuration of NFS","text":"<p>In Server /etc/exports file are the configuration file and using this we can able to export the directory to accessing on remote computer. ::</p> Bash Session<pre><code>vi /etc/exports\n</code></pre> <p>And Using below command we can able to export/sharing the directory to any other computer network user to access the directory files with read-only permission.</p> Bash Session<pre><code>echo '/home/nfs/ *(ro,sync)' &gt; /etc/exports\n</code></pre> <p>And also please find the below various other options to share the network file system on remote network user.</p> Entries in /etc/exports Configuration Description /home/nfs/ 192.168.1.55(rw,sync) /home/nfs/ directory accessible only for IP address 192.168.1.55 with read,write permissions, and synchronized mode /home/nfs/ 192.168.1.55(rw,sync,no_root_squash) /home/nfs directory accessible only for an IP address 192.168.1.55 with read,write permissions, synchronized mode and the remote root user will be treated as a root and will be able to change any file and directory /home/nfs/ 192.168.1.0/24(ro,sync) /home/nfs directory accessible for an 192.168.1.0 network with Subnet Mask 255.255.255.0 with read only permissions, and synchronized mode /home/nfs/ 192.168.1.55(rw,sync) 192.168.1.10(ro,sync) /home/nfs directory accessible for an IP address 192.168.1.55 with read, write permissions, and synchronized mode,and for an IP address 192.168.1.10 with read only permission and synchronize mode. /home/nfs/ technobureau(rw,sync) /home/nfs/ directory accessible only for an Hostname technobureau with read,write permissions, and synchronized mode /home/nfs/ *.technobureau.com(rw,sync) /home/nfs/ directory accessible for any hostname under technobureau.com with read and write permissions, and synchronized mode. <p>Once export configuration file are edited and then need to restart the nfs service to reflect the changes.</p> Ubuntu/DebianCentOS/RHEL Bash Session<pre><code>service nfs-kernel-server restart\n</code></pre> Bash Session<pre><code>service nfs restart\n</code></pre>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#client-configuration-of-nfs","title":"Client Configuration of NFS","text":"<p>Once successfully server setup and need to access the filesystem shared by the server from the client through network.And depending upon the export option client would got permission to access network file system directory and files available on network file system.</p> <p>Before we need to mount the network file system ,first we need an mount point and need to create by command mentioned below.</p> Bash Session<pre><code>mkdir /home/nfslocal\n</code></pre> <p>Then we need to mount the network system with the server ip and server hostname by mentioned below. And let us assume server IP as 192.168.1.50 ::</p> Bash Session<pre><code>mount 192.168.1.50:/home/nfs /home/nfslocal\n</code></pre> <p>Also we can able to mount the network file system by speficy the type of file system by mentioned below.</p> <p>Bash Session<pre><code>mount -t nfs 192.168.1.50:/home/nfs /home/nfslocal\n</code></pre> We can pass the nfs version to mount when server version is different from client.Let us assume client nfs version is 3 and use this command to mount the network file system.</p> Bash Session<pre><code>mount -t nfs -o nfsvers=3 192.168.1.50:/home/nfs /home/nfslocal\n</code></pre> <p>Warning</p> <p>Kindly note that this mounted network file system will not be accesible once client rebooted and need to mount again to get access the same.</p>"},{"location":"linux/overview/network_file_system_overview_and_configuration/#common-nfs-mount-options","title":"Common NFS Mount Options","text":"<ul> <li> <p>fsid=num - Forces the file handle and file attributes settings on the wire to be num, instead of a number derived from the major and minor number of the block device on the mounted file system. The value 0 has special meaning when used with NFSv4. NFSv4 has a concept of a root of the overall exported file system. The export point exported with fsid=0 is used as this root.</p> </li> <li> <p>hard - It doesn't wait for NFS server to communicate.User cannot terminate the process waiting for NFS communication to resume unless intr option is also specified.</p> </li> <li> <p>soft - Wait for NFS Server to communicate in-case of connectivity loss untill the timout period which is specified in timeo= option in seconds.Then once after timeout period exceeded and it would report an error. <li> <p>intr - Allows NFS requests to be interrupted if the server goes down or cannot be reached.</p> </li> <li> <p>nfsvers= - Able to mention the NFS version in-case of NFS version differences between the server.And also able to use which version to be used for this mount where multiple NFS version installed.It is not supported on NFSv4. <li> <p>noacl - To Turns of all ACL processing. This may be needed when interfacing with older version of Redhat or Solaris because recent ACL is not campatible with older version on those.</p> </li> <li> <p>nolock - To disable file locking in NFS.It is required when connecting with older version of NFS server.</p> </li> <li> <p>noexec - To Prevent execution of binaries on mounted file system.</p> </li> <li> <p>nosuid - To disable ser-user-identifier or set-group-identifier bits.This prevents remote users from gaining higherr privileges by running a setuid program.</p> </li> <li> <p>port= - Specifies the numeric value of NFS Server port when NFS server port has changed from default. If num is 0(default),then mount queries the remote host's portmapper for the port number which need to be use.If the remote host's NFS daemon is not registered with its portmapper,then standard NFS port number 2049 is used insted. <li> <p>rsize= and wsize= - This would be used to speed up the read/write communication of NFS by mentioning a larger data block size in bytes from default block size to be transfer on each time.Default value of NFSv2&amp;3 are 8192 and 32768 for NFSv4. <li> <p>tcp - By Specifying for NFS mount to use TCP protocol.</p> </li> <li> <p>udp - By Specifying for NFS mount to use UDP protocol.</p> </li> <li> <p>sec= - To specify the security protocol for NFS mount connection.And modes are    #. sys - default UNIX UID/GID for authentication.And it is used by default where nothing specified when mounting NFS.    #. krb5 - To Use Kerberos V5 authentication.    #. krb5i - To Use Kerberos V5 authentication and performs integrity.    #. krb5p - To Use Kerberos V5 authentication and prevents traffic sniffing.and also performs integrity to prevent data tampering."},{"location":"linux/overview/network_file_system_overview_and_configuration/#for-automount-of-nfs","title":"For Automount of NFS","text":"<p>To autmatic mount the network file system ,need to add entry on fstab to get access once after every restart. And kindly make sure NFS Server should accessible once before booting into the client system.Else it may fail to access the same. ::</p> Bash Session<pre><code>echo \"10.1.1.50:/home/nfs /home/nfs_local/ nfs defaults 0 0\" &gt;&gt; /etc/fstab\n</code></pre>"},{"location":"linux/overview/openssh_overview_and_usage/","title":"OpenSSH - Overview and Usage","text":"","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#what-is-secure-shellssh","title":"What is Secure Shell(SSH) ?","text":"<p>Secure Shell is an application protocol and software suite that allows secure network services over an insecure network such as the public internet by strong encryption and authentication. It can be used for remote terminal connections,remote file copy and forwarding X11 sessions as well as arbitrary TCP ports through a secure tunnel.It replaces the other insecure protocols such as Telnet and FTP.</p> <p>OpenSSH is a free and open-source set of computer tools that are used to provide secure and encrypted communication over a computer network using the ssh protocol. OpenSSH is commonly used to connect to a remote Linux shell, but it can also be used for other purposes, such as file transfer, tunneling, and port forwarding.</p> Service Name Port Number Transport Protocol ssh 22 TCP/UDP/SCTP <p>Remote accessing of Linux PCs from another Linux PC or server is an essential capability that is required in many scenarios, especially for system administrators and developers. It allows users to access a remote server, execute commands, transfer files, and perform other essential tasks from the comfort of their workstation or laptop. Fortunately, the process of accessing a Linux PC or server remotely is straightforward and can be accomplished using a set of computer tools called OpenSSH.</p> <p>Many people new to computers and protocols tend to create a misconception about OpenSSH, thinking it is a protocol. However, it is not. Instead, it is a set of computer programs that use the ssh protocol. The ssh protocol is a set of standards for secure and reliable communication between two computers over a network. It provides a secure and encrypted channel for data transfer, making it suitable for remote access to Linux PCs and servers.</p>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#installation-of-openssh","title":"Installation of OpenSSH","text":"<p>OpenSSH is a client-server application tool and server and client utilities need to be installed to get access through SSH protocol. openssh-server need to be installed on Server-End. And openssh-client need to be installed on Client-End to access the server.</p> <p>To install OpenSSH, you need to have superuser permissions on the remote server. The installation process varies depending on the Linux distribution you are using. On Ubuntu/Debian/Linux Mint, you can install OpenSSH by running the following command in the terminal.</p>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#openssh-server-installation","title":"OpenSSH Server installation","text":"Ubuntu/DebianRHEL/Centos/Fedora Bash Session<pre><code>sudo apt-get install openssh-server\n</code></pre> Bash Session<pre><code>yum -y install openssh-server\n</code></pre>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#openssh-client-installation","title":"OpenSSH Client installation","text":"Ubuntu/DebianRHEL/Centos/Fedora Bash Session<pre><code>sudo apt-get install openssh-client\n</code></pre> Bash Session<pre><code>dnf -y install openssh-client\n</code></pre>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#configuration","title":"Configuration","text":"<p>Configurations files are located under the /etc/ssh Directory and configurations files are mentioned below.</p> <p>!! note     Server Configuration File - /etc/ssh/sshd_config     Client Configurations File - /etc/ssh/ssh_config or ~/.ssh/config</p> <p>And here are few configuration and its usage are :</p> <ul> <li> <p><code>Host *</code>- Restricts the following declarations (up to the next Host or Match keyword) to be only for those hosts that match one of the patterns given after the Host keyword. In this case it allows all the host access.</p> </li> <li> <p>ConnectTimeout 0 - If Connection Timeout value defined then it used when connecting to the SSH server instead of using default system TCP timeout.And value only used for target is down or unreachable condition,not when server refuses the connection.</p> </li> <li> <p>Port 13 - Specifies the port number which need to used to connect the remote host. The default Port number is 22.</p> </li> <li> <p>AddressFamily - Specifies which address family to use when connecting.Valid arguments are \u201cany\u201d, \u201cinet\u201d (use IPv4 only), or \u201cinet6\u201d (use IPv6 only).</p> </li> <li> <p>BatchMode - If this option set to \u201cyes\u201d, passphrase/password querying will be disabled.</p> </li> <li> <p>ServerAliveInterval - If this used to maintain the server connectivity session to desired period instead of default 300 seconds when there is no active jobs running on the connection.</p> </li> <li> <p>BindAddress - Use the specified address on the local machine as the source address of the connection.Only useful on systems with more than one address. Note that this option does not work if UsePrivilegedPort is set to \u201cyes\u201d.</p> </li> <li> <p>Compression - Specifies whether to use compression.  The argument must be \u201cyes\u201d or \u201cno\u201d.  The default is \u201cno\u201d.</p> </li> <li> <p>CompressionLevel</p> <p>Specifies the compression level to use if compression is enabled.The argument must be an integer from 1 (fast) to 9(slow,best). The default level is 6, which is good for most applications.The meaning of the values is the same as in gzip(1).Note that this option applies to protocol version 1 only.</p> </li> <li> <p>ConnectionAttempts</p> <p>Specifies the number of tries (one per second) to make before exiting.The argument must be an integer.This may be useful in scripts if the connection sometimes fails.The default is 1.</p> </li> <li> <p>ConnectTimeout</p> <p>Specifies the timeout (in seconds) used when connecting to the SSH server, instead of using the default system TCP timeout. This value is used only when the target is down or really unreachable, not when it refuses the connection.</p> </li> <li> <p>GlobalKnownHostsFile</p> <p>Specifies one or more files to use for the global host key data\u2010base, separated by whitespace.The default is /etc/ssh/ ssh_known_hosts, /etc/ssh/ssh_known_hosts2.</p> </li> <li> <p>UserKnownHostsFile</p> <p>Specifies one or more files to use for the user host key database, separated by whitespace. The default is ~/.ssh/known_hosts, ~/.ssh/known_hosts2.</p> </li> <li> <p>HostbasedAuthentication</p> <p>Specifies whether to try rhosts based authentication with public key authentication.The argument must be \u201cyes\u201d or \u201cno\u201d.The default is \u201cno\u201d.  This option applies to protocol version 2 only and is similar to RhostsRSAAuthentication.</p> </li> <li> <p>NumberOfPasswordPrompts</p> <p>Specifies the number of password prompts before giving up.The argument to this keyword must be an integer.The default is 3.</p> </li> <li> <p>PasswordAuthentication</p> <p>Specifies whether to use password authentication. The argument to this keyword must be \u201cyes\u201d or \u201cno\u201d.The default is \u201cyes\u201d.</p> </li> <li> <p>PreferredAuthentications</p> <p>Specifies the order in which the client should try protocol 2 authentication methods.This allows a client to prefer one method (e.g. keyboard-interactive) over another method (e.g.password).The default is:( gssapi-with-mic,hostbased,publickey, keyboard-interactive,password ).</p> </li> <li> <p>Protocol</p> <p>Specifies the protocol versions ssh(1) should support in order of preference.The possible values are \u20181\u2019 and \u20182\u2019.Multiple versions must be comma-separated.When this option is set to \u201c2,1\u201d.ssh will try version 2 and fall back to version 1 if version 2 is not available.The default is \u20182\u2019.</p> </li> <li> <p>StrictHostKeyChecking</p> <p>If this flag is set to \u201cyes\u201d, ssh(1) will never automatically add host keys to the ~/.ssh/known_hosts file, and refuses to connect to hosts whose host key has changed.  This provides maximum protection against trojan horse attacks, though it can be annoying when the /etc/ssh/ssh_known_hosts file is poorly maintained or when connections to new hosts are frequently made.This option forces the user to manually add all new hosts.If this flag is set to \u201cno\u201d, ssh will automatically add new host keys to the user known hosts files.If this flag is set to \u201cask\u201d, new host keys will be added to the user known host files only after the user has confirmed that is what they really want to do, and ssh will refuse to connect to hosts whose host key has changed. The host keys of known hosts will be verified automatically in all cases.The argument must be \u201cyes\u201d, \u201cno\u201d, or \u201cask\u201d.The default is \u201cask\u201d.</p> </li> </ul>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#login-procedure-windows","title":"Login Procedure - Windows","text":"<p>Once the OpenSSH server is installed on the remote Linux PC/Server, we can use a Windows application like PuTTY to access it. If you don't have PuTTY installed on your system, you can download and install it from the official website.</p> <p>To log in to the remote Linux PC/Server using PuTTY, follow these steps:</p> <ol> <li>Open PuTTY and select \"ssh\" as the connection type.</li> <li>Enter the hostname or the remote server's DNS name or remote IP in the Host Name field. For example, hostname: technobureuau.com</li> <li>Enter your identity information, including your username and password.</li> <li>Download and install WinSCP or FileZilla application if you don't have it, to transfer files between your Windows and Linux PC/Server.</li> </ol>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#login-procedure-linux","title":"Login Procedure - Linux","text":"<p>To access the remote Linux PC/Server from another Linux PC/Server, you need to install the OpenSSH client on the source system. This will allow you to log in to the remote system from the source system.</p> <ul> <li>To log in to the remote Linux shell, open a terminal and type:</li> </ul> <p>Bash<pre><code>ssh -X &lt;your_username&gt;@&lt;host_name&gt;\n</code></pre> The hostname is the remote server's DNS name or remote IP, such as technobureau.com. You will be asked to enter the password, which you can simply type and press Enter. Once you are logged in, you can execute commands, transfer files, and perform other essential tasks on the remote server.</p> <ul> <li>To transfer files between the local and remote Linux PC or server, you can use the scp command. The scp command allows you to copy files securely between two Linux computers. To copy files to the server, run the following command on your workstation or laptop:</li> </ul> Bash<pre><code>scp -r &lt;path_from_directory&gt; &lt;your_username&gt;@&lt;host_name&gt;:&lt;path_to_directory&gt;\n</code></pre> <ul> <li>To copy files from the server, run the following command on your workstation or laptop:</li> </ul> Bash<pre><code>scp -r &lt;your_username&gt;@&lt;host_name&gt;:&lt;path_from_directory&gt; &lt;path_to_directory&gt;\n</code></pre>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/openssh_overview_and_usage/#conclusion","title":"Conclusion","text":"<p>OpenSSH is a set of computer tools that make remote accessing of Linux PCs from another Linux PC or server possible. Installing OpenSSH is a straightforward process that involves using superuser permissions to run commands on the terminal. By installing OpenSSH, you can access a remote Linux PC from a Windows machine or another Linux PC or server.</p>","tags":["Linux","SSH","OpenSSH"]},{"location":"linux/overview/raid_overview_and_functionalities/","title":"RAID - Overview and Functionality","text":"","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#introduction","title":"Introduction","text":"<p>RAID (Redundant Array of Independent Disks) is a storage virtualization technology that enhances data redundancy and performance by creating a logical unit of storage using multiple disk drive components. This document provides an in-depth explanation of RAID, including its various levels, such as RAID 0 and RAID 1, and their respective benefits in terms of reliability, availability, performance, and capacity.</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#raid-levels","title":"RAID Levels","text":"<p>RAID levels are specific schemes or architectures that determine how data is distributed across drives. Each RAID level offers a different balance of the key goals mentioned above. RAID levels higher than RAID 0 provide protection against unrecoverable read errors and complete disk failures. The different RAID levels include:</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#raid-0-data-striping","title":"RAID 0 - Data Striping","text":"<p>RAID 0 utilizes data striping, which involves dividing logically sequential data and distributing it across multiple storage devices. This technique significantly improves input/output (I/O) performance but does not provide redundancy.</p> <ul> <li>Excellent performance (Striping)</li> <li>Data Blocks are striped between the disks.</li> <li>Minimum of 2 disks</li> <li>No Redundancy/Mirror/Parity.</li> <li>Not advisable for any critical system.</li> </ul>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#raid-1-data-mirroring","title":"RAID 1 - Data Mirroring","text":"<p>RAID 1 involves data mirroring, where the same set of data blocks is stored across multiple storage devices. This redundancy technique ensures that if one storage device fails within a mirrored group, the data remains accessible.</p> <ul> <li>Excellent Redundancy (Mirroring)</li> <li>Good Performance</li> <li>Minimum of 2 disks</li> <li>No Striping/ Parity.</li> </ul>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#raid-parity-data-parity","title":"RAID Parity - Data Parity","text":"<p>RAID parity employs the technique of storing parity information on dedicated disks or distributing it across all disks in a RAID group. Parity information represents the XOR sum of the elements stored on the remaining disks in the RAID. In the event of a single disk failure, the lost data can be reconstructed by subtracting the remaining data across the RAID set.</p> <ul> <li>Good Performance (Data Striping)</li> <li>Good redundancy (Distributed parity)</li> <li>Minimum of 3 disks.</li> <li>No Mirroring.</li> </ul>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#benefits-of-raid","title":"Benefits of RAID","text":"<p>By implementing RAID, organizations can achieve the following advantages:</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#improved-data-redundancy","title":"Improved Data Redundancy:","text":"<p>RAID levels with redundancy, such as RAID 1 and those higher than RAID 1, provide protection against data loss due to disk failures. Redundancy ensures that even if a disk becomes inaccessible, the data remains intact and accessible from other disks in the array.</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#enhanced-performance","title":"Enhanced Performance","text":"<p>RAID, particularly RAID 0, significantly improves data access and transfer speeds by distributing data across multiple drives. This parallelization of data retrieval enhances overall system performance and reduces response times.</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#increased-storage-capacity","title":"Increased Storage Capacity","text":"<p>RAID allows the combination of multiple physical drives into a single logical unit, providing an aggregated storage capacity. By utilizing larger drives or adding more drives to the array, organizations can expand their overall storage capacity.</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/raid_overview_and_functionalities/#conclusion","title":"Conclusion","text":"<p>RAID technology offers a flexible and efficient approach to storage virtualization by combining multiple disks into a logical unit. By understanding the different RAID levels, organizations can choose the appropriate configuration based on their specific requirements for data redundancy, performance, and capacity. Implementing RAID can result in improved data protection, faster data access, and increased storage capacity, contributing to enhanced system reliability and efficiency.</p>","tags":["RAID","Data Mirroring","Disk Performance","Disk Redundancy"]},{"location":"linux/overview/understanding_swap_in_linux/","title":"Understanding Swap Space in Linux: Functionality, Benefits, and Recommended Allocation","text":"","tags":["Linux Swap","Swap Memory"]},{"location":"linux/overview/understanding_swap_in_linux/#introduction","title":"Introduction","text":"<p>Swap space is a crucial component in a Linux system that serves as a backup memory when the available RAM (Random Access Memory) is fully utilized. It provides additional memory capacity for running programs efficiently, preventing system slowdowns and interruptions. This documentation aims to explain the purpose and advantages of swap space, as well as provide guidance on its configuration.</p>","tags":["Linux Swap","Swap Memory"]},{"location":"linux/overview/understanding_swap_in_linux/#functionality-of-swap-space","title":"Functionality of Swap Space","text":"<p>When the system's RAM reaches its maximum capacity, inactive pages of memory are moved to the swap space, allowing the system to continue running programs in parallel without disruptions. Swap space acts as a supplement to RAM, enabling the system to handle memory demands beyond its physical limits.</p> <p>While swap space can help the system accommodate additional memory requirements, it is important to note that it does not replace the need for RAM. RAM remains the ideal and fastest hardware for memory operations, while swap space resides on the slower hard disk.</p>","tags":["Linux Swap","Swap Memory"]},{"location":"linux/overview/understanding_swap_in_linux/#memory-management-and-prioritization","title":"Memory Management and Prioritization","text":"<p>Linux employs prioritization strategies to manage the movement of data between RAM and swap space, optimizing overall system performance. The concept of \"swapiness\" determines the likelihood of items being moved to swap space. A higher swapiness value indicates that more items are prone to be relocated to swap space, freeing up memory for more critical operations.</p> <p>For example, during the startup phase of an application, a significant number of pages might be utilized for initialization and subsequently become inactive. These pages are assigned higher swapiness and are more likely to be moved to swap space, freeing up RAM for other applications.</p>","tags":["Linux Swap","Swap Memory"]},{"location":"linux/overview/understanding_swap_in_linux/#importance-of-swap-space-in-hibernation","title":"Importance of Swap Space in Hibernation","text":"<p>Swap space plays a crucial role in the hibernation process of a Linux system. When a system hibernates, the contents of memory are stored in the swap space. Therefore, without a swap partition, hibernation is not possible. It is worth noting that the need for hibernation is relatively rare among regular users.</p>","tags":["Linux Swap","Swap Memory"]},{"location":"linux/overview/understanding_swap_in_linux/#recommended-swap-space-configuration","title":"Recommended Swap Space Configuration","text":"<p>The appropriate allocation of swap space depends on the amount of RAM installed in the system. Here are the recommended minimum swap space configurations based on different RAM capacities:</p> <ul> <li>4GB of RAM or less: Minimum 2GB of swap space</li> <li>4GB to 16GB of RAM: Minimum 4GB of swap space</li> <li>16GB to 64GB of RAM: Minimum 8GB of swap space</li> <li>64GB to 256GB of RAM: Minimum 16GB of swap space</li> <li>256GB to 512GB of RAM: Minimum 32GB of swap space</li> </ul> <p>It is generally recommended to allocate swap space that is double the size of the existing RAM. However, these recommendations can be adjusted based on specific system requirements and usage patterns.</p>","tags":["Linux Swap","Swap Memory"]},{"location":"linux/overview/understanding_swap_in_linux/#conclusion","title":"Conclusion","text":"<p>Swap space is an essential component of a Linux system that provides additional memory capacity when the available RAM is fully utilized. It ensures the smooth operation of programs without interruptions, albeit at a slower speed compared to RAM. By understanding the functionality and benefits of swap space, system administrators can optimize their system's performance and configure swap space accordingly.</p>","tags":["Linux Swap","Swap Memory"]},{"location":"tools/","title":"Tools","text":"<p>where cutting-edge technology meets seamless efficiency. Explore our collection of powerful DevOps tools designed to revolutionize your workflows and unleash the full potential of your projects. From orchestrating complex workflows to automating repetitive tasks, we have the perfect solutions to elevate your development process.</p> <p>Take a breath of fresh air with Airflow, a robust and flexible tool that empowers you to effortlessly manage and schedule your data pipelines. With Airflow, you can gracefully navigate through the intricate web of dependencies, ensuring smooth execution and seamless coordination of your tasks. It's like having a skilled conductor orchestrating a symphony of data, all within your control.</p> <p>Step into the realm of automation with Ansible, your faithful companion in streamlining IT processes. With Ansible's simple yet powerful language, you can automate everything from configuration management to application deployment. It's like having an army of tireless helpers at your fingertips, ready to execute your commands with precision and reliability.</p> <p>Embark on a journey of seamless collaboration with Git, the backbone of modern software development. Git empowers you to track changes, merge code, and collaborate effortlessly with your team. It's like having a trusted version control system that never lets you down, ensuring the integrity of your codebase and facilitating seamless collaboration.</p> <p>Unleash the power of containerization with Docker, the ultimate tool for building, shipping, and running applications. With Docker, you can encapsulate your software and its dependencies into portable and lightweight containers. It's like having a magic box that effortlessly transports your applications across different environments, ensuring consistency and reproducibility wherever they go.</p> <p>Boost your testing capabilities with Selenium, the go-to tool for automated web testing. Selenium allows you to write robust and reliable tests that simulate real user interactions. It's like having a tireless QA engineer tirelessly clicking buttons and filling forms, ensuring your web applications perform flawlessly.</p> <p>Discover the art of infrastructure as code with Terraform, a powerful tool for provisioning and managing cloud resources. With Terraform's declarative language, you can define your infrastructure as code and effortlessly spin up and tear down resources across different cloud providers. It's like having a master architect blueprinting your cloud infrastructure, ensuring scalability and maintainability at every step.</p> <p>At our core, we believe in providing you with the most effective and innovative tools, empowering you to take control of your DevOps journey. Join us and unlock the true potential of your projects with our exceptional selection of tools.</p>"},{"location":"tools/airflow/","title":"Airflow Tools","text":"<p>Apache Airflow, the open-source airflow management platform, is a majestic and powerful tool that orchestrates the graceful movement of data and workflows within the digital realm. Like a master conductor, Apache Airflow seamlessly blends the art of automation with the science of data management, enabling organizations to achieve new heights of efficiency and productivity.</p> <p>Picture Apache Airflow as a gentle yet authoritative gust of wind, softly propelling data through a meticulously designed network of interconnected tasks and dependencies. With its flexible and extensible architecture, Airflow empowers users to construct intricate pipelines that harmoniously weave together diverse systems, applications, and data sources, transcending the boundaries of traditional data management.</p> <p>With Apache Airflow, the complexities of managing complex workflows and data pipelines become a captivating symphony. It offers a robust and intuitive interface that allows users to compose, visualize, and monitor their workflows, gaining a holistic understanding of the entire data orchestration process. Like an artist wielding a paintbrush, users can create and shape their data landscapes, making informed decisions and driving innovation with every stroke.</p> <p>This orchestration tool not only choreographs the movement of data but also elegantly handles the nuances of task scheduling, monitoring, and error handling. It provides a rich set of operators and sensors, acting as the conductor's baton, guiding each task's execution and monitoring their progress in real-time. Apache Airflow's fault-tolerant nature ensures that even in the face of adversity, data flows continue with resilience and reliability.</p> <p>Apache Airflow's true power lies in its extensibility and the vibrant community that surrounds it. It serves as a beacon, attracting data engineers, developers, and enthusiasts to contribute and enhance its capabilities. With a wide range of plugins and integrations, Airflow adapts to diverse technological landscapes, amplifying its potential and enabling organizations to leverage their existing tools and infrastructure seamlessly.</p> <p>In the ever-evolving realm of data engineering, Apache Airflow stands as an icon of innovation and adaptability. Its open-source heritage fosters collaboration and knowledge sharing, allowing practitioners to push the boundaries of what's possible in data orchestration. From startups to enterprises, Apache Airflow empowers organizations to harness the winds of data, unleashing their full potential and charting a course towards success in the digital era.</p>","tags":["Tools","Airflow"]},{"location":"tools/airflow/#see-also","title":"See also","text":"<ul> <li>Apache Airflow Docs</li> </ul>","tags":["Tools","Airflow"]},{"location":"tools/ansible/","title":"Ansible Tools","text":"<p>Ansible, the innovative marvel of automation, transcends the boundaries of conventional orchestration tools with its elegance and efficiency. Like a symphony conductor, it harmonizes the cacophony of diverse systems, weaving a tapestry of control and coordination.</p> <p>Picture a digital maestro poised at the podium, conducting a ballet of tasks across vast networks. With Ansible's ethereal touch, mundane configurations transcend into choreographed movements, effortlessly executed across myriad nodes. It empowers the conductor with an intuitive language, a conductor's baton, enabling orchestration symphonies that reverberate throughout the infrastructure.</p> <p>Ansible's allure lies in its versatility, a Swiss Army knife for automation aficionados. Like an enchanting chameleon, it adapts to any ecosystem, effortlessly bridging the gaps between different platforms, operating systems, and cloud environments. Whether it's provisioning, deployment, or configuration management, Ansible deftly navigates the labyrinthine pathways of complexity, ensuring a seamless and consistent performance.</p> <p>Simplicity is Ansible's secret weapon. Its incantations, known as \"playbooks,\" akin to musical scores, elegantly express the desired state of the system. These melodic incantations bring the gift of idempotence, ensuring systems dance to the same tune, no matter the number of times played.</p> <p>Ansible's prowess lies in its agentless nature, an invisible puppeteer orchestrating the digital realm from behind the scenes. It whispers commands and whispers alone, imbuing it with a subtle charm and a non-intrusive presence. Its sorcery empowers administrators to weave complex automation spells without the burden of additional software on target systems.</p> <p>In the realm of infrastructure as code, Ansible reigns supreme. Like a master sculptor, it molds virtual infrastructures, breathing life into data centers and cloud deployments. Infrastructure becomes malleable clay, shaped effortlessly by Ansible's hands, delivering agility, scalability, and resilience to the realms it governs.</p> <p>Ansible, the transformative enchantress of automation, transcends mere tooling, becoming an indispensable companion on the journey to a more efficient and scalable digital landscape. With its harmonious symphony of simplicity, adaptability, and power, Ansible dances at the forefront of the automation revolution, leaving a trail of awe-inspired technologists in its wake.</p>","tags":["Tools","Ansible"]},{"location":"tools/ansible/#see-also","title":"See also","text":"<ul> <li>Ansible Docs</li> </ul>","tags":["Tools","Ansible"]}]}